{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 32572,
     "sourceId": 3934,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "lastEditStatus": {
   "notebookId": "vfgie63mb5rtvyrbtixk",
   "authorId": "2460170691448",
   "authorName": "USER",
   "authorEmail": "",
   "sessionId": "70d8b561-2d3e-4230-8fe9-30be819fea32",
   "lastEditTime": 1750297779030
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ddb995b-9816-4408-8241-daf2201edadf",
   "metadata": {
    "collapsed": false,
    "name": "E2E_ML"
   },
   "source": "# End-to-End ML: Deep Learning Recommendation Model\nこのレコメンデーション・エンジンは、Tasty Bytesのフードトラックが販売する各メニューについて、ロイヤルティ顧客にレコメンデーション・スコアを提供する。この出力は、パーソナライズされたアウトリーチ、顧客が訪問するトラックブランドの増加、業績不振のトラックへのトラフィックの増加に使用される。\n\nレコメンデーション・エンジンのモデル学習は、GPUデバイス間での分散学習を活用しており、エンドツーエンドのモデル開発とデプロイは、以下のSnowflake機能を使用して簡素化および合理化されています：\n- Snowflake Notebooks with GPU Container Runtime (GA)\n- Snowflake Feature Store (GA)\n- Snowflake Modeling API (GA) - Preprocessing, Training (PyTorch API), Evaluation\n- Snowflake Model Registry (GA)\n- Model Deployment from Registry to SPCS (GA)"
  },
  {
   "cell_type": "markdown",
   "id": "565d3ba2-7a1a-4f15-b0fe-d5670559b354",
   "metadata": {
    "collapsed": false,
    "name": "SETUP"
   },
   "source": "## Setup\nEnd-to-Endのモデル開発とデプロイに必要となるSnowflakeライブラリをインポート"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "installs",
    "language": "python",
    "collapsed": false
   },
   "outputs": [],
   "source": "!pip install snowflake-ml-python==1.8.0\n!pip install snowflake-snowpark-python==1.29.0\n!pip install torchvision==0.18.1\n!pip install torch==2.3.1",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "id": "8b26c5b8-4940-4ae1-86b2-4e2e3ca23a21",
   "metadata": {
    "collapsed": false,
    "name": "setup2"
   },
   "source": "### GPU Device Info\nノートブックで利用可能なGPUデバイスの数を表示\n\n**Snowflake Feature**: Snowflake GPU Notebooks (GA) - SnowflakeノートブックからGPUリソースに簡単にアクセスできます。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe3796-ca94-4e00-95f1-a73436ca05d3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "setup3"
   },
   "outputs": [],
   "source": "import torch\n# Get device info\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(\"Number of GPU devices available:\", num_gpus)\n    \n    for i in range(num_gpus):\n        print(\"Device\", i, \":\", torch.cuda.get_device_name(i))\n    \n    #Set a default device\n    torch.cuda.set_device(0)\nelse:\n    print(\"CUDA is not available. Check your installation or GPU setup.\")"
  },
  {
   "cell_type": "markdown",
   "id": "80d6c542-a65f-4892-afc3-1bcc26a4fec8",
   "metadata": {
    "name": "setup_feature_store_md",
    "collapsed": false
   },
   "source": "## 特徴量ストアの作成\nSnowflake特徴量ストアは、データサイエンスおよび機械学習ワークロードにおける特徴量の作成、保存、管理をより簡単かつ効率的に行えるように設計されています。プロデューサは、フィーチャーストア内のフィーチャービューとエンティティを管理し、部門間のコラボレーションとフィーチャーの再利用を促進します。このアプローチにより、トレーニング環境とサービング環境間のサイロが解消され、Pythonコマンドを使用した集計や移動ウィンドウの計算が簡素化されます。"
  },
  {
   "cell_type": "markdown",
   "id": "f8fb0247-abed-4532-bba9-80a84570ba5c",
   "metadata": {
    "name": "Import_Packages",
    "collapsed": false
   },
   "source": "Import the necessary packages for development."
  },
  {
   "cell_type": "code",
   "id": "a10029e6-94f7-4bfb-8a3c-1b181517de13",
   "metadata": {
    "language": "python",
    "name": "Importing_Packages",
    "collapsed": false
   },
   "outputs": [],
   "source": "import os\nimport time\nimport math\n#import sys\n\n# Third-party library imports\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif \n\n# Snowflake library imports\nimport streamlit as st\nfrom snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.pytorch import (\n    PyTorchTrainer,\n    ScalingConfig,\n    WorkerResourceConfig,\n)\n#from snowflake.ml.modeling.data import MLRuntimeDataset\n#from snowflake.ml.data.data_connector import DataConnector\nfrom snowflake.ml.modeling.distributors.pytorch import PyTorchDistributor, PyTorchScalingConfig, WorkerResourceConfig\nfrom snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n\n# from snowflake.ml.modeling.pytorch.context import getContext\nfrom snowflake.ml.modeling.distributors.pytorch import get_context\n\nfrom snowflake.ml.feature_store import (\nFeatureStore,\nFeatureView,\nEntity,\nCreationMode\n)\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.modeling.metrics import (\nroc_auc_score,  \nprecision_score, \nrecall_score, \nconfusion_matrix\n)\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Add a query tag to the session. This helps with debugging and performance monitoring.\nsession.query_tag = {\"origin\":\"sf_sit\", \"name\":\"tasty_bytes_e2e_ml\", \"version\":{\"major\":1, \"minor\":0}, \"attributes\":{\"is_quickstart\":0, \"source\":\"notebook\"}}\n\ndb = str(session.get_current_database().strip('\"'))\nsolution_prefix = (db.upper()).split('_PROD')[0]\nimport warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eac511c6-e472-429c-bf85-1b47261d8df2",
   "metadata": {
    "name": "Feature_Store_Schemas",
    "collapsed": false
   },
   "source": "### 特徴量ストアのスキーマと権限\nフィーチャストア・スキーマを作成し、プロデューサがフィーチャストア・ビューを作成するために必要なすべての権限と、コンシューマがこれらのビューにアクセスするための権限を付与します。"
  },
  {
   "cell_type": "code",
   "id": "b27bded2-6ddf-4ebd-abe3-2a8c427a9192",
   "metadata": {
    "language": "sql",
    "name": "Feature_Store_Schema_Privileges",
    "collapsed": false
   },
   "outputs": [],
   "source": " -- $FS_ROLE_PRODUCER create feature views and $FS_ROLE_CONSUMER uses the feature views for training. These roles are created as part of the setup\nUSE ROLE ACCOUNTADMIN;\nSET FS_ROLE_PRODUCER = '{{solution_prefix}}_FS_PRODUCER';\nSET FS_ROLE_CONSUMER = '{{solution_prefix}}_DATA_SCIENTIST';\nSET FS_DATABASE = '{{solution_prefix}}_PROD';\nSET FS_SCHEMA = 'FS_SCHEMA';\nSET SCHEMA_FQN = CONCAT($FS_DATABASE, '.', $FS_SCHEMA);\nSET FS_WAREHOUSE = '{{solution_prefix}}_DS_WH';\nSET MR_DEMO_DB='{{solution_prefix}}_PROD';\n\n-- Create schema\nCREATE SCHEMA IF NOT EXISTS IDENTIFIER($FS_SCHEMA);\n\n-- Build role hierarchy\nGRANT ROLE IDENTIFIER($FS_ROLE_CONSUMER) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n\n-- Grant PRODUCER role privileges\nGRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT CREATE DYNAMIC TABLE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT CREATE VIEW ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT CREATE TAG ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\nGRANT APPLY TAG ON ACCOUNT TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n\n-- Grant CONSUMER role privileges\nGRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT SELECT,MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT SELECT,MONITOR ON ALL DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT SELECT,REFERENCES ON FUTURE VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT SELECT,REFERENCES ON ALL VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n\n-- [Optional] Grant USAGE ON WAREHOUSE to CONSUMER\nGRANT USAGE ON WAREHOUSE IDENTIFIER($FS_WAREHOUSE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\nGRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f020aef5-dbff-4f30-8d39-5301135a2726",
   "metadata": {
    "name": "Feature_Store",
    "collapsed": false
   },
   "source": "Feature Store Producer Roleを使用してFeature Storeを定義します。"
  },
  {
   "cell_type": "code",
   "id": "ca21eca6-b6e2-4a2e-a38c-f7247521fe75",
   "metadata": {
    "language": "python",
    "name": "fsdefinition",
    "collapsed": false
   },
   "outputs": [],
   "source": "session.sql(f'USE ROLE {solution_prefix}_FS_PRODUCER')\nsession.sql(f'USE WAREHOUSE {solution_prefix}_DS_WH').collect()\nsession.sql('USE SCHEMA FS_SCHEMA').collect()\n\nFS=FeatureStore(\nsession=session,\ndatabase=f\"{solution_prefix}_PROD\",\n    name=\"FS_SCHEMA\",\n    default_warehouse=f\"{solution_prefix}_DS_WH\",\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ac0e47b-e7d1-4cf6-8a54-89ecdbcd31bf",
   "metadata": {
    "language": "python",
    "name": "menu",
    "collapsed": false
   },
   "outputs": [],
   "source": "menu_spdf = session.sql(\"SELECT menu_type, truck_brand_name, menu_item_name, item_category, item_subcategory, sale_price_usd FROM raw_pos.menu WHERE item_category != 'Beverage'\");\nmenu_spdf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05e7b229-068c-43f1-b0a9-f5bd4b8d2253",
   "metadata": {
    "language": "python",
    "name": "cust",
    "collapsed": false
   },
   "outputs": [],
   "source": "cust_spdf = session.sql(\"SELECT customer_id, city, country, gender, marital_status, birthday_date, DATEDIFF(year, birthday_date, CURRENT_DATE()) AS age FROM raw_customer.customer_loyalty\");\nst.dataframe(cust_spdf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8f422ecb-0ced-410d-9713-c3c3054f1b36",
   "metadata": {
    "language": "python",
    "name": "avg_monthly",
    "collapsed": false
   },
   "outputs": [],
   "source": "avg_monthly_purchase_amount = session.sql(f\"SELECT  customer_id, ROUND(SUM(order_total) / (TIMESTAMPDIFF(MONTH, MIN(date), MAX(date)) + 1),2) AS avg_monthly_purchase_amount FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\")\navg_monthly_purchase_amount",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "981d673f-bcac-4a7d-8301-4ac0271677dc",
   "metadata": {
    "language": "python",
    "name": "avg_weekly",
    "collapsed": false
   },
   "outputs": [],
   "source": "avg_weekly_purchase_amount = session.sql(f\"SELECT customer_id,ROUND(SUM(order_total) / (TIMESTAMPDIFF(WEEK, MIN(date), MAX(date)) + 1),2) AS avg_weekly_purchase_amount FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\");\navg_weekly_purchase_amount",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e106a8f4-016a-47db-8fd1-7b78d868a48c",
   "metadata": {
    "language": "python",
    "name": "avgyearly",
    "collapsed": false
   },
   "outputs": [],
   "source": "avg_yearly_purchase_amount = session.sql(f\"SELECT customer_id, ROUND(SUM(order_total)/(TIMESTAMPDIFF(YEAR, MIN(date), MAX(date)) + 1),2) AS avg_yearly_purchase_amount FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\")\navg_yearly_purchase_amount",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2786587-6792-41f2-838d-8aebf3d73527",
   "metadata": {
    "language": "python",
    "name": "joined_avgs",
    "collapsed": false
   },
   "outputs": [],
   "source": "cust_avgs_spdf= avg_monthly_purchase_amount.join(avg_weekly_purchase_amount,\"CUSTOMER_ID\").join(avg_yearly_purchase_amount,\"CUSTOMER_ID\")\nst.dataframe(cust_avgs_spdf)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21a2f5ab-23d5-4f26-bdb8-9f5abdf16275",
   "metadata": {
    "name": "entities",
    "collapsed": false
   },
   "source": "特徴量ストアのエンティティ作成"
  },
  {
   "cell_type": "code",
   "id": "78b6e0c3-3ed8-48f1-886d-2d10a935d1f4",
   "metadata": {
    "language": "python",
    "name": "create_entities",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Snowflake Feature Store requires an \"entity\" with \"join_keys\" be registered\ncustentity = Entity(name=\"CustomerIds\", join_keys=[\"Customer_ID\"])\nFS.register_entity(custentity)\n\nMenuentity = Entity(name=\"Menu_ItemNames\", join_keys=[\"MENU_ITEM_NAME\"])\nFS.register_entity(Menuentity)\n\n\nPurchaseavgs_entity = Entity(name=\"Purchase_Avgs\", join_keys=[\"Customer_ID\"])\nFS.register_entity(Purchaseavgs_entity)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b1c2e8b-9ce9-402a-9a3a-173b95451039",
   "metadata": {
    "language": "python",
    "name": "list_entities",
    "collapsed": false
   },
   "outputs": [],
   "source": "FS.list_entities().show(100)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "571284f6-bc4e-46de-930e-121d90255ad7",
   "metadata": {
    "name": "register1",
    "collapsed": false
   },
   "source": "顧客特徴量Viewの登録"
  },
  {
   "cell_type": "code",
   "id": "1c6fb20d-6f33-4e65-9b58-3d8b7cec1eb7",
   "metadata": {
    "language": "python",
    "name": "register_cust",
    "collapsed": false
   },
   "outputs": [],
   "source": "fv = FeatureView(\n    name=\"CUSTOMER_FEATURES\",\n    entities = [custentity],\n    feature_df=cust_spdf,\n    refresh_freq=\"1 day\"\n)\nregistered_fv = FS.register_feature_view(\n    feature_view=fv,\n    version=\"V1\",\n     block=True,\n    overwrite=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26945111-715a-43e4-ae9d-6e1cc45ea055",
   "metadata": {
    "name": "register2",
    "collapsed": false
   },
   "source": "メニュー特徴量Viewの登録"
  },
  {
   "cell_type": "code",
   "id": "c917ff2a-7f7d-4744-9057-941a730017e8",
   "metadata": {
    "language": "python",
    "name": "register_menu",
    "collapsed": false
   },
   "outputs": [],
   "source": "fv = FeatureView(\n    name=\"MENU_FEATURES\",\n    entities = [Menuentity],\n    feature_df=menu_spdf,\n    refresh_freq=\"1 day\"\n)\n\nregistered_fv = FS.register_feature_view(\n    feature_view=fv,\n    version=\"V1\",\n    block=True,\n    overwrite=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef0bf5aa-dd59-44d3-a486-e25272ea1f25",
   "metadata": {
    "name": "register3",
    "collapsed": false
   },
   "source": "購入特徴量Viewの登録"
  },
  {
   "cell_type": "code",
   "id": "eb7bd435-191d-416b-975e-5b9ba7608933",
   "metadata": {
    "language": "python",
    "name": "register_purch",
    "collapsed": false
   },
   "outputs": [],
   "source": "fv = FeatureView(\n    name=\"PURCHASE_FEATURES\",\n    entities = [Purchaseavgs_entity],\n    feature_df=cust_avgs_spdf,\n    refresh_freq=\"1 day\"\n)\n\nregistered_fv = FS.register_feature_view(\n    feature_view=fv,\n    version=\"V1\",\n     block=True,\n    overwrite=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b202ee8b-1c1c-44e2-a033-aa4a6d181209",
   "metadata": {
    "language": "python",
    "name": "feature_views",
    "collapsed": false
   },
   "outputs": [],
   "source": "FS.list_feature_views(entity_name=\"Purchase_Avgs\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5bd7e8e5-5845-45dd-bbae-382292965e3a",
   "metadata": {
    "name": "access_features",
    "collapsed": false
   },
   "source": "## 特徴量ストアから特徴量へアクセス"
  },
  {
   "cell_type": "markdown",
   "id": "d96bdca0-0795-47f4-b062-ff638eb59f31",
   "metadata": {
    "collapsed": false,
    "name": "FEATURE_STORE"
   },
   "source": "特徴量ストアには、顧客、メニューアイテム、購入の特徴量ビューが含まれます。モデル特徴量は特徴量ストアからアクセスします。\n\n**Snowflakeの特長：** 特徴量ストア（GA） - データと連動する特徴量を簡単に見つけることができます。"
  },
  {
   "cell_type": "code",
   "id": "9f7f48ad-6c28-42cb-ab0a-13aa5264f311",
   "metadata": {
    "language": "sql",
    "name": "change_role",
    "collapsed": false
   },
   "outputs": [],
   "source": "USE ROLE {{solution_prefix}}_DATA_SCIENTIST;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b30d0-0ea7-4c90-a260-6574c924cfc0",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_store1"
   },
   "outputs": [],
   "source": [
    "# Access feature views\n",
    "FS=FeatureStore(\n",
    "session=session,\n",
    "database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\")\n",
    "\n",
    "customer_fv : FeatureView = FS.get_feature_view(\n",
    "    name='CUSTOMER_FEATURES',\n",
    "    version='V1'\n",
    ")\n",
    "print(customer_fv)\n",
    "\n",
    "menu_fv : FeatureView = FS.get_feature_view(\n",
    "    name='MENU_FEATURES',\n",
    "    version='V1'\n",
    ")\n",
    "print(menu_fv)\n",
    "\n",
    "purchase_fv : FeatureView = FS.get_feature_view(\n",
    "   name='PURCHASE_FEATURES',\n",
    "   version='V1'\n",
    ")\n",
    "print(purchase_fv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f4158-e371-48d6-937e-056f9b869073",
   "metadata": {
    "collapsed": false,
    "name": "features_store2"
   },
   "source": "### 特徴量ストアからデータセットの作成\nインタラクションデータセットには、各メニュー・顧客ペアの購入フラグが含まれる。このインタラクションデータセットはトレーニング、検証、テストに分割される。特徴は特徴ストアからデータセットに持ち込まれる。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052221d-915e-4c1c-9909-bf63321d8287",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_store3"
   },
   "outputs": [],
   "source": [
    "# Split the interaction dataset and get features from the feature store\n",
    "def create_dataset(spine_df, name):\n",
    "    data = FS.generate_dataset(\n",
    "    name=name,\n",
    "    spine_df=spine_df,\n",
    "    features=[customer_fv, menu_fv, purchase_fv]\n",
    "    )\n",
    "    df = data.read.to_snowpark_dataframe().drop(\"BIRTHDAY_DATE\")\n",
    "    return df\n",
    "    \n",
    "interaction_df = session.table('analytics.loyalty_purchased_items')\n",
    "\n",
    "# Split into train/validation/test\n",
    "datasets = interaction_df.random_split([.1, .1, .8])\n",
    "\n",
    "# Build training tables\n",
    "train_df = create_dataset(datasets[0], \"train\")\n",
    "val_df = create_dataset(datasets[1], \"validation\")\n",
    "    \n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d38fd-ea11-4959-822a-29d93f529410",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "TrainingsetCnt"
   },
   "outputs": [],
   "source": [
    "train_df.count() + val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcf59d-19fb-4c78-8372-1dcd56702ef2",
   "metadata": {
    "collapsed": false,
    "name": "FEATURE_ENGINEERING"
   },
   "source": "## 特徴量エンジニアリング\nこのモデルは、カテゴリ（スパース）特徴の埋め込みを作成する。カテゴリ値は一意な整数としてエンコードされる。前処理として、疎な特徴にはラベル符号化を、数値（密な）特徴には最小-最大スケーリングを適用します。\n\n**Snowflakeの特徴:** Snowpark ML Modeling API - 特徴エンジニアリングと前処理（GA） - 頻繁に使用されるscikit-learnの前処理関数を分散実行することで、パフォーマンスとスケーラビリティを向上。"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44e7b770-5544-4902-bb42-dabbef01cb2e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-01-02T09:50:51.140175Z",
     "iopub.status.busy": "2024-01-02T09:50:51.139850Z",
     "iopub.status.idle": "2024-01-02T09:50:51.146147Z",
     "shell.execute_reply": "2024-01-02T09:50:51.145066Z",
     "shell.execute_reply.started": "2024-01-02T09:50:51.140147Z"
    },
    "language": "python",
    "name": "feature_engineering1",
    "trusted": true
   },
   "outputs": [],
   "source": "# Preprocess sparse and dense features\nsparse_features = ['MENU_ITEM_NAME', \n                   'MENU_TYPE', \n                   'TRUCK_BRAND_NAME', \n                   'ITEM_CATEGORY', \n                   'ITEM_SUBCATEGORY',\n                   'CITY',\n                   'COUNTRY',\n                   'GENDER',\n                   'MARITAL_STATUS',]\n\ndense_features = ['SALE_PRICE_USD',\n                  'AGE',\n                  'AVG_MONTHLY_PURCHASE_AMOUNT',\n                  'AVG_WEEKLY_PURCHASE_AMOUNT',\n                  'AVG_YEARLY_PURCHASE_AMOUNT',\n                 ]\n\nlabel_col = \"PURCHASED\"\n\n# Create pipeline\npipeline_steps = []\n\n# Label encode sparse features\nfor i, feat in enumerate(sparse_features):\n    le_step = (\n        f\"LE{i+1}\",\n        LabelEncoder(input_cols=[feat], output_cols=[feat]),\n    )\n    pipeline_steps.append(le_step)\n\n# Scale dense features\npipeline_steps.append(\n    (\n        \"MMS\",\n        MinMaxScaler(\n            feature_range=(0, 1),\n            input_cols=dense_features,\n            output_cols=dense_features\n        )\n    )\n)\n\n# Preprocessing pipeline\npreprocessing_pipeline = Pipeline(steps=pipeline_steps)\ntrain_data = preprocessing_pipeline.fit(train_df).transform(train_df)\nval_data = preprocessing_pipeline.transform(val_df)"
  },
  {
   "cell_type": "code",
   "id": "48abc5cc-a16e-4f44-91e7-388d7836df9c",
   "metadata": {
    "language": "python",
    "name": "save_train_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "train_data.write.mode(\"overwrite\").save_as_table(\"ml.train_data_table\", table_type=\"temporary\")\ntrain_data.write.mode(\"overwrite\").save_as_table(\"ml.val_data_table\", table_type=\"temporary\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4049f14d-6c28-4d45-a288-d98383fb520c",
   "metadata": {
    "collapsed": false,
    "name": "feature_engineering2"
   },
   "source": "### パイプラインの保存\n保存されたパイプラインは、推論における特徴量変換にて利用"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad088f-a487-4b8b-86ee-ec966c004655",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "feature_engineering3"
   },
   "outputs": [],
   "source": [
    "# Save pipeline to a stage where it can be centrally accessed\n",
    "pipeline_local_path = f'/tmp/dlrm_preprocessor_v1.joblib'\n",
    "joblib.dump(preprocessing_pipeline, open(pipeline_local_path, 'wb'))\n",
    "session.file.put(pipeline_local_path, \n",
    "                 '@ML.ML_STAGE/dlrm_preprocessor_v1.joblib', \n",
    "                 auto_compress=False, \n",
    "                 overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a2d3f-887a-4d8e-a072-cd281373e18a",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "label_encoders1"
   },
   "outputs": [],
   "source": [
    "USE SCHEMA ML;\n",
    "CREATE or replace STAGE UDF_STAGE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc20b90-29fd-4c5b-ae4b-b1e3d6a747d4",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "label_encoders2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = train_df[dense_features + sparse_features + [label_col]]\n",
    "data = data.with_columns(sparse_features,\n",
    "                        [F.col(c).cast(T.StringType()) for c in sparse_features])\n",
    "\n",
    "def serialize_label_encoders(label_encoders):\n",
    "    serialized_label_encoders = {}\n",
    "    for feat, lbe in label_encoders.items():\n",
    "        serialized_label_encoders[feat] = {\n",
    "            'input_cols': lbe.input_cols,\n",
    "            'output_cols': lbe.output_cols,\n",
    "            'classes_': lbe.classes_.tolist()\n",
    "        }\n",
    "    return serialized_label_encoders\n",
    "\n",
    "def save_label_encoders_to_stage(label_encoders, stage_name, dir_name):\n",
    "        serialized_label_encoders = json.dumps(label_encoders)\n",
    "        # Write serialized encoders to a local file first\n",
    "        with open('/tmp/label_encoders.json', 'w') as f:\n",
    "            f.write(serialized_label_encoders)\n",
    "        # Upload the local file to the Snowflake stage\n",
    "        session.file.put('/tmp/label_encoders.json', f'@{stage_name}/{dir_name}',auto_compress=False)\n",
    "        return f'Uploaded to @{stage_name}/{dir_name}'\n",
    "    \n",
    "label_encoders = {}\n",
    "\n",
    "# Iterate over each sparse feature\n",
    "for feat in sparse_features:\n",
    "    # Initialize LabelEncoder for the current feature\n",
    "    lbe = LabelEncoder(input_cols=[feat], output_cols=[feat+'_ENCODED'],drop_input_cols=True)\n",
    "    \n",
    "    # Fit LabelEncoder to the data\n",
    "    lbe.fit(data)\n",
    "    \n",
    "    # Store the LabelEncoder object for reference\n",
    "    label_encoders[feat] = lbe\n",
    "    data = lbe.transform(data)\n",
    "# Serialize label encoders\n",
    "serialized_label_encoders = serialize_label_encoders(label_encoders)\n",
    "stage_name=\"UDF_STAGE\"\n",
    "dir_name=\"dlrm_label_encoders\"\n",
    "# Save serialized label encoders to a file\n",
    "with open('/tmp/label_encoders.json', 'w') as f:\n",
    "    json.dump(serialized_label_encoders, f)\n",
    "save_label_encoders_to_stage(serialized_label_encoders, stage_name, dir_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b75eca0-ad84-4681-83a7-a66e36a00b4f",
   "metadata": {
    "collapsed": false,
    "name": "MODEL"
   },
   "source": "## モデル定義\nこのPyTorchモデルは深層学習推薦モデル（DLRM）である。これは、各ロイヤリティ顧客に全てのメニューの推薦スコアを提供するために使用されている。\n- 埋め込み層はカテゴリ特徴を密なベクトルに変換する。\n- 数値的特徴は多層パーセプトロン（MLP）層を通して処理される。\n- 特徴相互作用層は入力特徴のペア間の複雑な関係を捉える。\n- 最後のdence層は推薦スコアを生成する。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeae81-114e-479c-a6b5-c6bd5d7facde",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "model1"
   },
   "outputs": [],
   "source": [
    "# PyTorch DLRM\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature_dim = inputs.shape[1]\n",
    "        concat_features = inputs.view(-1, feature_dim, 1)\n",
    "        dot_products = torch.matmul(concat_features, concat_features.transpose(1, 2))\n",
    "        ones = torch.ones_like(dot_products) \n",
    "        mask = torch.triu(ones)\n",
    "        out_dim = feature_dim * (feature_dim + 1) // 2\n",
    "        flat_result = dot_products[mask.bool()]\n",
    "        reshape_result = flat_result.view(-1, out_dim)\n",
    "        return reshape_result\n",
    "\n",
    "class DLRM(nn.Module):\n",
    "    \n",
    "    def __init__(self, sparse_feature_number, dense_feature_number, num_embeddings, embed_dim, bottom_mlp_dims, top_mlp_dims):\n",
    "        super(DLRM, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.EmbeddingBag(num_embeddings=num_embeddings, embedding_dim=embed_dim, mode='sum')\n",
    "        self.layer_feature_interaction = FeatureInteraction()\n",
    "        \n",
    "        self.bottom_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(dense_feature_number, bottom_mlp_dims[0]),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(bottom_mlp_dims[0], bottom_mlp_dims[1]),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        top_mlp_input_dim = (\n",
    "            (embed_dim + bottom_mlp_dims[1]) \n",
    "            * ((embed_dim + bottom_mlp_dims[1]) + 1) // 2 \n",
    "            + bottom_mlp_dims[1]\n",
    "         )\n",
    "\n",
    "        self.top_mlp = nn.Sequential(\n",
    "            nn.Linear(top_mlp_input_dim, top_mlp_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[0], top_mlp_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(top_mlp_dims[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        # Embedding layer for categorical inputs\n",
    "        embed_x = self.embeddings(x_sparse)\n",
    "        # MLPs for numeric inputs\n",
    "        bottom_mlp_output = self.bottom_mlp(x_dense)\n",
    "        # Combine categical embeddings and MLP outputs\n",
    "        concat_first = torch.cat([bottom_mlp_output, embed_x], dim=-1)\n",
    "        # Get feature interactions\n",
    "        interaction = self.layer_feature_interaction(concat_first)\n",
    "        # Concat interaction outputs with MLP outputs\n",
    "        concat_second = torch.cat([interaction, bottom_mlp_output], dim=-1)\n",
    "        # MLP layers to output \n",
    "        output = self.top_mlp(concat_second)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fac44-6cef-4f6e-9b88-dd4466974e7d",
   "metadata": {
    "collapsed": false,
    "name": "TRAINING"
   },
   "source": "## モデルトレーニング\n\nモデルトレーニング機能は、各デバイスにモデルとデータを配置する。勾配は各トレーニングバッチで結合され、すべてのデバイスに伝搬される。各エポックの後、トレーニングと検証の損失がすべてのデバイスで平均化され、モデルの重みが保存されます。\n\n**Snowflakeの特長:** Snowpark ML Modeling API - PyTorch - Snowpark DataFrameからGPUデバイスに分散して実行。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe955bed-61be-463a-bef7-fd1ed225cdbd",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "training1"
   },
   "outputs": [],
   "source": [
    "# Adjust number of epochs and records in the training data\n",
    "num_epochs = 2\n",
    "training_sample = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbdc98-d252-46f4-a787-9b176b5a5acb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "training2"
   },
   "outputs": [],
   "source": "# Model training function\ndef setup(rank, world_size):\n    # Initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    torch.manual_seed(42)\n\ndef train_func():\n    \n    context = get_context()\n    rank = context.get_rank()\n    world_size = context.get_world_size()\n    setup(rank, world_size)\n\n    batch_size = 256\n\n    # GET DATA FROM CONTEXT AND SET UP TENSORS\n    dataset_map = context.get_dataset_map()\n    training_data = dataset_map[\"train\"].get_shard().to_torch_dataset(\n        batch_size=batch_size, shuffle=True\n    )\n    validation_data = dataset_map[\"val\"].get_shard().to_torch_dataset(\n        batch_size=batch_size, shuffle=True\n    )\n    dataloader = DataLoader(training_data, batch_size=None)\n    val_dataloader = DataLoader(validation_data, batch_size=None)\n\n    # DEFINE MODEL\n    model = DLRM(\n        sparse_feature_number=len(sparse_features),\n        dense_feature_number=len(dense_features),\n        num_embeddings=142,\n        embed_dim=128,\n        bottom_mlp_dims=[256, 128],\n        top_mlp_dims=[128, 128],\n    )\n        \n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.001)\n    \n    # TRAIN\n    for epoch in range(num_epochs):\n        start_time = time.time()\n        records_processed = 0\n        running_loss = 0.0\n        i = 0\n        \n        for batch_idx, batch_data in enumerate(dataloader):\n            y = batch_data.pop(label_col).type(torch.float32).to(rank).squeeze()\n            \n            x_sparse = torch.stack(\n                [tensor.to(torch.int).squeeze() for key, tensor in batch_data.items() if key in sparse_features],\n                dim=1\n            )\n            x_dense = torch.stack(\n                [tensor.to(torch.float32).squeeze() for key, tensor in batch_data.items() if key in dense_features],\n                dim=1\n            )\n                        \n            optimizer.zero_grad()\n            output = ddp_model(x_sparse, x_dense)\n            loss = criterion(output, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            records_processed += len(y)\n\n            if (batch_idx + 1) % 500 == 0:\n                print(\n                    f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}: Device {rank} processed {records_processed} records, Epoch Time: {time.time() - start_time:.2f} seconds, Average Training loss: {running_loss / (batch_idx + 1):.4f}\"\n                )\n\n        print(\n            f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx + 1}: Device {rank} processed {records_processed} records, Epoch Time: {time.time() - start_time:.2f} seconds, Average Training loss: {running_loss / (batch_idx + 1):.4f}\"\n        )\n\n        # Average loss across devices\n        running_loss_tensor = torch.tensor(running_loss / (batch_idx + 1), device=rank)\n        dist.all_reduce(running_loss_tensor)\n        dist.barrier()\n        running_loss = running_loss_tensor.item()\n        running_loss /= world_size\n\n        # GET VALIDATION LOSS\n        ddp_model.eval()\n        val_loss = 0.0\n        for val_batch_idx, val_batch_data in enumerate(val_dataloader):\n            y_val = val_batch_data.pop(label_col).type(torch.float32).to(rank).squeeze()\n            x_sparse_val = torch.stack(\n                [tensor.to(torch.int).squeeze() for key, tensor in val_batch_data.items() if key in sparse_features],\n                dim=1\n            )\n            x_dense_val = torch.stack(\n                [tensor.to(torch.float32).squeeze() for key, tensor in val_batch_data.items() if key in dense_features],\n                dim=1\n            )\n    \n            with torch.no_grad():\n                output_val = ddp_model(x_sparse_val, x_dense_val)\n                loss_val = criterion(output_val, y_val.unsqueeze(1))\n            \n            val_loss += loss_val.item()\n    \n        # Average validation loss across devices\n        val_loss_tensor = torch.tensor(val_loss / (val_batch_idx + 1), device=rank)\n        dist.all_reduce(val_loss_tensor)\n        dist.barrier()\n        val_loss = val_loss_tensor.item()\n        val_loss /= world_size\n        ddp_model.train()\n        \n    \n        # SAVE MODEL\n        if rank == 0:\n            print(f\" Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss:.4f}, Validation Loss: {val_loss:.4f}, Epoch Time: {time.time() - start_time:.2f} seconds \")\n            torch.save(model.state_dict(), '/tmp/latest_model.pth')\n    \n    dist.destroy_process_group()  # Train - Snowflake ML PyTorch API"
  },
  {
   "cell_type": "code",
   "id": "2ca58471-f254-43e6-8a55-cb0a33e2d7b1",
   "metadata": {
    "language": "python",
    "name": "training3",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Train - Snowflake ML PyTorch API\npytroch_trainer = PyTorchDistributor(\n    train_func=train_func,\n    scaling_config=PyTorchScalingConfig(\n        num_nodes=1,\n        num_workers_per_node=1,\n        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=0, num_gpus=1),\n    ),\n)\n\ntrain_data = session.table(\"ml.train_data_table\")\nval_data = session.table(\"ml.val_data_table\")\ndata_train = ShardedDataConnector.from_dataframe(train_data.limit(training_sample))\ndata_val = ShardedDataConnector.from_dataframe(val_data)\n\nout = pytroch_trainer.run(\n    dataset_map=dict(\n             train=data_train,\n             val=data_val\n         )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "edf4af3f-13f9-42c4-ba09-d44189e85f84",
   "metadata": {
    "collapsed": false,
    "name": "DEPLOYMENT"
   },
   "source": "## モデルのデプロイ\nモデルは Snowflake Model Registry に記録されます。ログに記録されたモデルは、Snowpark Container Services (SPCS)上で推論のためにデプロイされます。\n\n**Snowflakeの特長**： Snowflake Model Registry with SPCS deployment - 柔軟なコンピュート環境において、Snowflakeでモデルとそのメタデータを安全にデプロイ、管理します。\n\n![model_serving](https://docs.snowflake.com/en/_images/model-registry-spcs-deployment.png)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a1dc2-021e-42ea-b980-8637b52d0120",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment1"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "def load_model(model_path):\n",
    "    model = DLRM(sparse_feature_number=len(sparse_features),\n",
    "                 dense_feature_number=len(dense_features),\n",
    "                 num_embeddings=142,\n",
    "                 embed_dim=128,\n",
    "                 bottom_mlp_dims=[256, 128],\n",
    "                 top_mlp_dims=[128, 128])\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load saved model\n",
    "model = load_model('/tmp/latest_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a78965-eabe-43c1-b761-6e7aa2858b56",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment2"
   },
   "outputs": [],
   "source": [
    "# Register the model to the Snowflake model registry.\n",
    "registry = Registry(session=session, database_name=f\"{solution_prefix}_PROD\", schema_name=\"REGISTRY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9044e8-4f2c-492f-99b2-737ebd84ad79",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment3"
   },
   "outputs": [],
   "source": "train_data = session.table(\"ml.train_data_table\")\nsample_input = train_data.limit(1).to_pandas()\nx_sparse = torch.tensor(sample_input[sparse_features].values, dtype=torch.int)\nx_dense = torch.tensor(sample_input[dense_features].values, dtype=torch.float32)\n\n# Log model to registry\nmodel_ref = registry.log_model(\n    model,\n    model_name=\"RecModelDemo\",\n    version_name=\"V1\",\n    pip_requirements=[\"torchvision==0.18.1\",\"torch==2.3.1\"],\n    conda_dependencies=[\"pyopenssl>=22.0.0\",\"libxml2<=2.13.7\"],\n    sample_input_data=[x_sparse[0].unsqueeze(0), x_dense[0].unsqueeze(0)],\n    options={'relax_version': False, 'multiple_inputs': True}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e98a0e-c524-4d11-99e8-dd63130c30a3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "deployment4",
    "vscode": {
     "languageId": "sql"
    },
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Deploying Model to Container Runtime using GPU's\nmodel_ref.create_service(service_name=\"TB_REC_SERVICE_DEMO_PREDICT\",\n                  service_compute_pool=f\"{solution_prefix}_DEPLOY_POOL\",\n                  image_repo=f\"{solution_prefix}_PROD.REGISTRY.IMAGE_REPO\",\n                  build_external_access_integration=f\"{solution_prefix}_CONDA_ACCESS_INTEGRATION\")"
  },
  {
   "cell_type": "markdown",
   "id": "791dea7c-2cd3-4023-b5d1-4e22ab69196e",
   "metadata": {
    "collapsed": false,
    "name": "INFERENCE"
   },
   "source": "## モデルの推論と検証\nテストデータの推論は、専用のコンピュートプール上で動作する SPCS にデプロイされたモデルを使用して完了する。テストデータのフィーチャーはフィーチャーストアからアクセスされ、前処理パイプラインが必要に応じてデータを変換します。\n\n**モデル出力**： \nモデルは入力特徴量に基づくスコアを出力する。スコアが高ければ高いほど、その顧客にとってそのメニューがよりおすすめであることを示します。モデルのパフォーマンスを評価するために、スコアからバイナリ予測が作成されます（スコアが0.5以上の場合は1、そうでない場合は0）。\n\n**スノーフレークの特徴：**\n- SPCS 上での推論（PuPr） - 専用のコンピュートプールを持つコンテナ環境にデプロイされたモデルに対して推論を実行します。\n- Snowpark ML Modeling API - Evaluation Metrics (GA) - 使用頻度の高い scikit-learn 前処理関数の分散実行により、パフォーマンスとスケーラビリティを向上。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e125048-5dc6-4a32-92ce-012f1f4a83ef",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "inference2"
   },
   "outputs": [],
   "source": "# Get features from feature store\ntest_df = FS.retrieve_feature_values(\n    spine_df=datasets[2],\nfeatures=[customer_fv, menu_fv, purchase_fv]\n)\n#test_df_subset = test_df.sample(100000)\ntest_df_subset = test_df.sample(n=10000)\n\n# Preprocess\ntest_data = preprocessing_pipeline.transform(test_df_subset)\n\n# To maintain the Order\ntest_data_pd = test_data.to_pandas()\n\nsparse_input = torch.tensor(test_data_pd[sparse_features].values, dtype=torch.int)\ndense_input = torch.tensor(test_data_pd[dense_features].values, dtype=torch.float32)\ninput_data = [sparse_input, dense_input]\n\npredictions = model_ref.run(input_data, function_name=\"forward\", service_name=\"TB_REC_SERVICE_DEMO_PREDICT\")\n\npredictions['output_feature_0'] = predictions['output_feature_0'].apply(\n    lambda x: x[0] if isinstance(x, list) else float(x)\n)\neval_df_pd = pd.concat([test_data_pd[[\"CUSTOMER_ID\", \"MENU_ITEM_NAME\", \"PURCHASED\"]], \n                       predictions.rename(columns={'output_feature_0': 'PREDICTION'})], axis = 1).assign(\n                                         BINARY_PREDICTION=lambda df: np.where(df['PREDICTION'] >= 0.5, 1, 0))\neval_df = session.create_dataframe(eval_df_pd)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bf322-f7bf-4078-ac69-d41303596b90",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "inference3"
   },
   "outputs": [],
   "source": "eval_df"
  },
  {
   "cell_type": "markdown",
   "id": "b43a30bf-2dce-4caf-838f-a3dc9381fa8f",
   "metadata": {
    "collapsed": false,
    "name": "EVALUATION"
   },
   "source": "### 検証\n\n強力な推薦モデルは、購入されたアイテムのほとんどを推薦するはずである（これは、あるアイテムが顧客にとって関心のあるものであることを示すトレーニング指標である）。一方、未購入のアイテムは、必ずしも興味がないことを示しているわけではない。推薦エンジンのゴールは、顧客が興味を持ちうる未購入アイテムを特定することであり、そのためには未購入アイテムを「誤分類」する必要がある。理想的には、想起を高め、未購入アイテムの一部を推薦することである。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f49dbd2-9728-443f-a562-8b65fc8318e7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "evaluation1"
   },
   "outputs": [],
   "source": "# Get Evaluation Metrics\ncols = st.columns(3)\ncols[0].metric(\"AUC\", round(roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\"),3))\ncols[1].metric(\"Recall\", round(recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\ncols[2].metric(\"Precision\", round(precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91698a5-2cb7-4183-8eb8-8f5b8295ede2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "evaluation2"
   },
   "outputs": [],
   "source": "m = registry.get_model(\"RecModelDemo\")\nmv=m.version(\"v1\")\nmv.set_metric(\"AUC\", round(roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\"),3))\nmv.set_metric(\"Recall\", round(recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\nmv.set_metric(\"Precision\", round(precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\"),3))\nm.description = \"Provides menu recommendations for Tasty bytes business\"\n"
  },
  {
   "cell_type": "markdown",
   "id": "c1330b94-eadf-4484-b03a-79c88848f327",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "### ユーザーインタラクションで推論実行"
  },
  {
   "cell_type": "code",
   "id": "576c451d-975f-4ef5-8ce3-5ed24c6677b7",
   "metadata": {
    "language": "python",
    "name": "st_functions"
   },
   "outputs": [],
   "source": "def get_filters():\n    filter_clause = ' 1=1 '\n    if filters['country'] is not None and len(filters['country']) > 0:\n        country_where_clause = \"(\"\n        for x in filters['country']:\n            country_where_clause += f\"'{x}',\"\n        country_where_clause = country_where_clause[:-1] + \")\"\n        filter_clause += \" and country in \" + country_where_clause\n\n    if filters['city'] is not None and len(filters['city']) > 0:\n        city_where_clause = \"(\"\n        for x in filters['city']:\n            city_where_clause += f\"'{x}',\"\n        city_where_clause = city_where_clause[:-1] + \")\"\n        filter_clause += \" and city in \" + city_where_clause\n\n    if filters['truck_brand_name'] is not None and len(filters['truck_brand_name']) > 0:\n        truck_brand_name_where_clause = \"(\"\n        for x in filters['truck_brand_name']:\n            truck_brand_name_where_clause += f\"'{x}',\"\n        truck_brand_name_where_clause = truck_brand_name_where_clause[:-1] + \")\"\n        filter_clause += \" and truck_brand_name in \" + truck_brand_name_where_clause\n    return filter_clause\n\ndef get_filtered_data():\n    with st.spinner(\"Getting Filtered Data...\"):\n        time.sleep(1)\n        cust_avgs = cust_avgs_spdf\n        item_df = session.table('ml.menu_item_features').filter(\"item_category != 'Beverage'\")\n        customer_df = session.table('ml.customer_features')\n        purchase_df = session.table('analytics.loyalty_purchased_items')\n        df = purchase_df.join(item_df, 'menu_item_name', 'left')\n        filters_clause = get_filters()\n        st.write(filters_clause)\n        result_df = df.join(customer_df, 'customer_id', 'left')\n        final_df = result_df.join(cust_avgs, 'customer_id', 'left').filter(filters_clause).order_by(\"customer_id\").limit(10000)\n    return final_df\n\ndef infer_model(test_data):\n    with st.spinner(\"Inferring Deep Learning Model on SPCS...\"):\n        time.sleep(1)\n\n        # Get deployed model\n        reg = Registry(session=session, database_name=session.get_current_database(), schema_name='REGISTRY')#, database_name='TEST_MAY_19_TASTYBYTESENDTOENDML_PROD', schema_name=\"REGISTRY\")\n        m = reg.get_model('RECMODELDEMO')\n        mv = m.version(\"v1\")\n    \n        test_data_pd = test_data.to_pandas()\n\n        # Build input tensor\n        sparse_input = torch.tensor(test_data_pd[sparse_features_encoded].values, dtype=torch.int)\n        dense_input = torch.tensor(test_data_pd[dense_features].values, dtype=torch.float32)\n        input_data = [sparse_input, dense_input]\n\n        # Run inference on deployed model\n        predictions = mv.run(\n            input_data,\n            function_name = \"FORWARD\",\n            service_name = \"TB_REC_SERVICE_DEMO_PREDICT\"\n        )\n\n        # Concat with input dataframe\n        predictions['output_feature_0'] = predictions['output_feature_0'].apply(\n            lambda x: x[0] if isinstance(x, list) else float(x)\n        )\n        recommendations = pd.concat([test_data_pd[[\"CUSTOMER_ID\", \"CITY\", \"MENU_ITEM_NAME\", \"PURCHASED\"]], \n                               predictions.rename(columns={'output_feature_0': 'PREDICTION'})], axis = 1)\n        recommendations_df = session.create_dataframe(recommendations)\n\n        # Define a window partitioned by customer_id and ordered by prediction score\n        windowSpec = Window.partitionBy(col(\"customer_id\")).orderBy(col(\"prediction\").desc())\n            \n        # Add a rank column to rank the predictions for each customer\n        rankedDf = recommendations_df.withColumn(\"rank\", rank().over(windowSpec))\n        \n        # Filter for rows where rank is 1 (top prediction for each customer)\n        # topPredictionsDf = rankedDf.filter(col(\"rank\") == 1)\n\n        # Filter for rows where rank is less than or equal to 3 (top 3 predictions for each customer)\n        topPredictionsDf = rankedDf.filter(col(\"rank\") <= 3)\n        \n        # Convert DataFrame to Pandas DataFrame\n        top_predictions_pd = topPredictionsDf.toPandas()\n        \n        # Group by customer_id and collect the top 3 recommendations as a list\n        grouped_top_predictions = top_predictions_pd.groupby(\"CUSTOMER_ID\").agg({'CITY': 'first','MENU_ITEM_NAME': lambda x: list(x)}).reset_index()\n        \n        distinct_customer_ids = grouped_top_predictions['CUSTOMER_ID'].unique()\n        sql_in_clause = \"\"\n        \n        # Loop through the distinct customer_ids to form the SQL IN clause\n        for customer_id in distinct_customer_ids:\n            # Append each customer_id to the SQL IN clause\n            sql_in_clause += \"'\" + str(customer_id) + \"', \"\n        \n        # Remove the trailing comma and space\n        sql_in_clause = sql_in_clause[:-2]\n        history_df = session.sql(f\"\"\"select customer_id, \n                                    menu_item_name as Purchase_History\n                                    from analytics.loyalty_purchased_items\n                                    where purchased = 1\n                                    and customer_id in ({sql_in_clause});\"\"\").to_pandas()\n        grouped_history_df = history_df.groupby('CUSTOMER_ID')['PURCHASE_HISTORY'].agg(list).reset_index()\n\n        # Merge finalDf_pd with grouped_history_df on customer_id\n        finalDf_pd = pd.merge(grouped_top_predictions, grouped_history_df, on='CUSTOMER_ID', how='left')\n\n        # Reorder columns\n        finalDf_pd = finalDf_pd[['CUSTOMER_ID', 'CITY', 'PURCHASE_HISTORY', 'MENU_ITEM_NAME']]\n\n        finalDf_pd['PURCHASE_HISTORY'] = finalDf_pd['PURCHASE_HISTORY'].apply(lambda x: x if isinstance(x, list) else [])\n    return finalDf_pd\n\ndef get_serialized_label_encoders():\n    stage_name = 'ml.UDF_STAGE'\n    dir_name='dlrm_label_encoders'\n    file_name = 'label_encoders.json'\n    \n    session.file.get(f'@{stage_name}/{dir_name}/{file_name}',\"/tmp/dir/\")\n    modified_label_encoders = {}\n    # Read the downloaded file content\n    with open(f\"/tmp/dir/label_encoders.json\", \"r\") as f:\n        serialized_content = f.read()\n    \n    # Deserialize the label encoders\n    serialized_label_encoders = json.loads(serialized_content)\n    for feat, params in serialized_label_encoders.items():\n        modified_params = params.copy()  # Create a copy to avoid modifying the original dictionary\n        # Update the classes_ list to create the desired output structure\n        modified_params['classes_'] = {i: v for i, v in enumerate(params['classes_'])}\n        modified_label_encoders[params['output_cols'][0]] = modified_params['classes_']\n    return modified_label_encoders\n    \ndef apply_label_encoding(df):\n    if 'modified_label_encoders' not in st.session_state:\n        st.session_state.modified_label_encoders = get_serialized_label_encoders()\n    serialized_label_encoders = st.session_state.modified_label_encoders\n    input_cols_to_drop = []\n    for feat, mapping in serialized_label_encoders.items():\n        # Get the input column from the serialized label encoder\n        input_col = feat[:-len(\"_ENCODED\")]  # Extract the column name from the encoded column name\n\n        # Get the mapping dictionary for the current feature\n        encoding_mapping = {int(k): v for k, v in mapping.items()}\n        \n        # Generate encoding for the input column\n        case_stmt = \"CASE\"\n        for encoded_val, class_val in encoding_mapping.items():\n            case_stmt += f\" WHEN {input_col} = '{class_val}' THEN {encoded_val}\"\n        case_stmt += \" ELSE NULL END\"\n\n        # Apply label encoding using the CASE statement\n        output_col = f\"{input_col}_encoded\"\n        df = df.withColumn(output_col, F.expr(case_stmt))\n\n        if input_col not in (\"CITY\", \"TRUCK_BRAND_NAME\", \"MENU_ITEM_NAME\"):\n            input_cols_to_drop.append(input_col)\n\n    # Drop the original input columns\n    df = df.drop(*input_cols_to_drop)\n    return df\n\n# Function to simulate processing features\ndef process_features(df):\n    with st.spinner(\"Processing Features...\"):\n        time.sleep(1)\n        df_encoded = apply_label_encoding(df)\n        mms = MinMaxScaler(feature_range=(0, 1), input_cols=dense_features, output_cols=dense_features)\n        mms.fit(df_encoded)\n        testdata = mms.transform(df_encoded)\n    return testdata",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abaa7123-b8b6-4af6-ad2a-fb97fbbaa02d",
   "metadata": {
    "language": "python",
    "name": "st_interaction"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark.functions import rank, col\nfrom snowflake.snowpark.window import Window\n\nsparse_features_encoded = ['MENU_ITEM_NAME_ENCODED',\n                           'MENU_TYPE_ENCODED',\n                           'TRUCK_BRAND_NAME_ENCODED',\n                           'ITEM_CATEGORY_ENCODED',\n                           'ITEM_SUBCATEGORY_ENCODED',\n                           'CITY_ENCODED',\n                           'COUNTRY_ENCODED',\n                           'GENDER_ENCODED',\n                           'MARITAL_STATUS_ENCODED']\n\n@st.cache_data(ttl=120)\ndef get_listdata():\n    country_list = session.sql(\"\"\"select distinct COUNTRY\n                                                from ml.customer_features \n                                                order by COUNTRY;\"\"\").to_pandas()\n    city_list = session.sql(\"\"\"select distinct CITY \n                                                from ml.customer_features \n                                                order by CITY;\"\"\").to_pandas()\n    truck_brand_name_list = session.sql(\"\"\"select distinct TRUCK_BRAND_NAME\n                                                        from ml.menu_item_features\n                                                        order by TRUCK_BRAND_NAME;\"\"\").to_pandas()\n    return country_list, city_list, truck_brand_name_list\n\ndef on_change_callback(key):\n    if key == \"country\":\n        filters['city'] = []\n        country_where_clause = \"\"\n        if len(filters['country']) > 0 :\n            country_where_clause += \"(\"\n            for x in filters['country']:\n                country_where_clause += f\"'{x}',\"\n            country_where_clause = country_where_clause[:-1] + \")\"\n            city_list = session.sql(f\"\"\"select distinct CITY \n                                                        from ml.customer_features \n                                                        where COUNTRY in {country_where_clause} \n                                                        order by CITY;\"\"\").to_pandas()\n        else:\n            city_list = session.sql(\"\"\"select distinct CITY \n                                                        from ml.customer_features \n                                                        order by CITY;\"\"\").to_pandas()\nsession = get_active_session()\ncountry_list, city_list, truck_brand_name_list = get_listdata()\nfilters = {}\n\ncol1, col2, col3 = st.columns(3, gap=\"medium\")\nfilters['country'] = col1.multiselect('対象国',\n                                       country_list,\n                                       on_change=on_change_callback,\n                                       key=\"country\",\n                                       kwargs={\"key\": \"country\"})\nfilters['city'] = col2.multiselect('対象都市',\n                                    city_list,\n                                    on_change=on_change_callback,\n                                    key=\"city\",\n                                    kwargs={\"key\": \"city\"})\nfilters['truck_brand_name'] = col3.multiselect('Truck Brand Name',\n                                                  truck_brand_name_list,\n                                                  on_change=on_change_callback,\n                                                  key=\"truck_brand_name\",\n                                                    kwargs={\"key\": \"truck_brand_name\"})\n\nif st.button(\"推論実行\"):\n    df = get_filtered_data()\n    data = process_features(df)\n    recommendations = infer_model(data)\n    recommendations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc295012-e9ef-4909-b8c6-7d5ba69e0fb8",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### タスクとして定期実行"
  },
  {
   "cell_type": "code",
   "id": "20a00e6e-ba86-4159-83b4-d348b652b3ca",
   "metadata": {
    "language": "python",
    "name": "task_sample"
   },
   "outputs": [],
   "source": "from snowflake.core._common import CreateMode\nfrom snowflake.core.task import Cron\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation, DAGTaskBranch\nfrom snowflake.snowpark import Session\n\n\ndef get_data_task(session: Session)-> None:\n    avg_monthly_purchase_amount = session.sql(f\"SELECT  customer_id, ROUND(SUM(order_total) / (TIMESTAMPDIFF(MONTH, MIN(date), MAX(date)) + 1),2) AS avg_monthly_purchase_amount FROM TASTYBYTESENDTOENDML_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\")\n    avg_weekly_purchase_amount = session.sql(f\"SELECT customer_id,ROUND(SUM(order_total) / (TIMESTAMPDIFF(WEEK, MIN(date), MAX(date)) + 1),2) AS avg_weekly_purchase_amount FROM TASTYBYTESENDTOENDML_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\")\n    avg_yearly_purchase_amount = session.sql(f\"SELECT customer_id, ROUND(SUM(order_total)/(TIMESTAMPDIFF(YEAR, MIN(date), MAX(date)) + 1),2) AS avg_yearly_purchase_amount FROM TASTYBYTESENDTOENDML_PROD.ANALYTICS.ORDERS_V GROUP BY customer_id\")\n    cust_avgs= avg_monthly_purchase_amount.join(avg_weekly_purchase_amount,\"CUSTOMER_ID\").join(avg_yearly_purchase_amount,\"CUSTOMER_ID\")\n    \n    cust_avgs = cust_avgs_spdf\n    item_df = session.table('ml.menu_item_features').filter(\"item_category != 'Beverage'\")\n    customer_df = session.table('ml.customer_features')\n    purchase_df = session.table('analytics.loyalty_purchased_items')\n    df = purchase_df.join(item_df, 'menu_item_name', 'left')\n    filters_clause = \"1=1\"\n    result_df = df.join(customer_df, 'customer_id', 'left')\n    final_df = result_df.join(cust_avgs, 'customer_id', 'left').filter(filters_clause).order_by(\"customer_id\").limit(10000)\n    session.write_pandas(final_df, table_name=\"customer_data\",overwrite=True, auto_create_table=True, table_type=\"\"temporary\"\")\n\ndef preprocess_task(session: Session) -> None:\n  pass  # do something\n\ndef infer_task(session: Session) -> str:\n  # do something\n  return \"task3\"\n\ntry:\n  with DAG(\n    \"my_dag\",\n    schedule=Cron(\"10 * * * *\", \"Asia/Tokyo\"),\n    stage_location=\"@UDF_STAGE\",\n    packages=[\"snowflake-snowpark-python\"],\n    use_func_return_value=True,\n  ) as dag:\n    task1 = DAGTask(\n      \"task1\",\n      get_data_task,\n      warehouse=test_warehouse,\n    )\n    task1_branch = DAGTaskBranch(\"task1_branch\", task_branch_handler, warehouse=test_warehouse)\n    task2 = DAGTask(\"task2\", preprocess_task, warehouse=test_warehouse)\n    task3 = DAGTask(\"task3\", infer_task, warehouse=test_warehouse, condition=\"1=1\")\n    task1 >> task1_branch\n  schema = root.databases[\"my_db\"].schemas[\"my_schema\"]\n  op = DAGOperation(schema)\n  op.deploy(dag, mode=CreateMode.or_replace)\nfinally:\n  session.close()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcd9b9d4-0576-472c-a4fb-4ea7d57e3a71",
   "metadata": {
    "collapsed": false,
    "name": "SUMMARY"
   },
   "source": "## 総括\nこのノートブックはGPUコンピュートプールを活用し、スケールの大きなデータとディープラーニングモデルを扱う能力を解き放ちました。エンドツーエンドで、このワークフローはSnowpark DataFrameを使用し、以下の機能を活用して開発とデプロイを簡素化しました：\n- GPUコンテナランタイム上でのSnowflakeノートブック\n- Snowflake フィーチャーストア\n- Snowflake Modeling API - 前処理、トレーニング（PyTorch API PuPr）、評価\n- Snowflakeモデルレジストリ\n- SPCSへのモデルデプロイメント"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python",
    "name": "close_session"
   },
   "outputs": [],
   "source": "session.close()",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  }
 ]
}