{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "caqv4aausnhucdf2ygeg",
   "authorId": "4469144185903",
   "authorName": "SKAWAKAMI",
   "authorEmail": "shinichi.kawakami@snowflake.com",
   "sessionId": "b87f64e3-38f6-490a-b48f-8be1d049ab8a",
   "lastEditTime": 1756442900874
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "intro_md",
    "collapsed": false
   },
   "source": "# Snowflake ML with PyTorch Distributor - クイックスタートガイド\n\nこのNotebookでは、PyTorch Distributorを使用したSnowflake MLの包括的な機械学習プラットフォームを活用して、エンドツーエンドのMLワークフローを構築します。\n\nこのレコメンデーション・エンジンは、Tasty Bytesのフードトラックが販売する各メニューについて、ロイヤルティ顧客にレコメンデーション・スコアを提供する。この出力は、パーソナライズされたアウトリーチ、顧客が訪問するトラックブランドの増加、業績不振のトラックへのトラフィックの増加に使用される。\nレコメンデーション・エンジンのモデル学習は、GPUデバイス間での分散学習を活用しており、エンドツーエンドのモデル開発とデプロイは、以下のSnowflake機能を使用して簡素化および合理化されています：\n\n## 学習内容\n\n- **Snowflake Notebooks with GPU Container Runtime (GA)**: Snowflake Notebooks\n- **Snowflake Feature Store (GA)** : 特徴量の一元管理と再利用 \n- **Snowflake Modeling API (GA)** : Preprocessing, Training (PyTorch API), Evaluation\n- **Snowflake Model Registry (GA)** : モデルのバージョン管理、メタデータ管理、デプロイメント\n- **Model Deployment from Registry to SPCS (GA)** : モデルの推論サービス\n\n## 構築するもの\n\n以下を使用してエンドツーエンドの深層学習推薦システムを構築します：\n- GPU加速による分散PyTorch学習\n- Snowflake Feature Storeを使用した特徴量エンジニアリング\n- Snowpark Container Services (SPCS)へのモデルデプロイメント\n- 包括的なモデル監視と説明可能性\n",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "setup_md"
   },
   "source": [
    "## 1. 環境セットアップとGPU確認\n",
    "\n",
    "Container Runtime環境では、必要なパッケージが事前にインストールされています。まずGPUの利用可能性を確認します。\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "setup",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU デバイス情報の確認\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"✅ 利用可能なGPUデバイス数: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(f\"   デバイス {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # デフォルトデバイスを設定\n",
    "    torch.cuda.set_device(0)\n",
    "    print(f\"   デフォルトデバイス: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"❌ CUDAが利用できません。GPU設定を確認してください。\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## 2. 必要なライブラリのインポート\n",
    "\n",
    "Container Runtime環境では、ML開発に必要なパッケージが事前にインストールされています。\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "lib_import",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 標準ライブラリ\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# サードパーティライブラリ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Snowflakeライブラリ\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "# Snowflake ML\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.distributors.pytorch import (\n",
    "    PyTorchDistributor, \n",
    "    PyTorchScalingConfig, \n",
    "    WorkerResourceConfig,\n",
    "    get_context\n",
    ")\n",
    "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
    "from snowflake.ml.modeling.metrics import (\n",
    "    roc_auc_score,  \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"✅ すべてのライブラリが正常にインポートされました！\")\n",
    "print(f\"PyTorchバージョン: {torch.__version__}\")\n",
    "print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "initialize_md"
   },
   "source": [
    "## 3. Snowflakeセッションの初期化\n",
    "\n",
    "アクティブなSnowflakeセッションを取得し、クエリタグを設定します。\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "initialize",
    "language": "python"
   },
   "outputs": [],
   "source": "# アクティブセッションの取得\nsession = get_active_session()\n\n# クエリタグの設定（デバッグとパフォーマンス監視に役立ちます）\nsession.query_tag = {\n    \"origin\": \"pytorch_ml_quickstart\",\n    \"name\": \"end_to_end_recommendation_system\",\n    \"version\": {\"major\": 1, \"minor\": 0},\n    \"attributes\": {\"is_quickstart\": 1, \"source\": \"notebook\"}\n}\n\n# データベース情報の取得\ndb = str(session.get_current_database().strip('\"'))\nsolution_prefix = (db.upper()).split('_PROD')[0]\n\nprint(f\"✅ Snowflakeセッション初期化完了\")\nprint(f\"   データベース: {db}\")\nprint(f\"   ソリューションプレフィックス: {solution_prefix}\")\nprint(f\"   現在のロール: {session.get_current_role()}\")\nprint(f\"   現在のウェアハウス: {session.get_current_warehouse()}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "setup_feature_store_md"
   },
   "source": [
    "## 4. Feature Storeのセットアップ\n",
    "\n",
    "Snowflake Feature Storeは、データサイエンスと機械学習ワークロードのための特徴量の作成、保存、管理をより簡単で効率的にするように設計されています。\n",
    "\n",
    "**主要なメリット:**\n",
    "- **一元化された特徴量管理**: 特徴量を一箇所で保存・管理\n",
    "- **特徴量の再利用**: チームやプロジェクト間での特徴量の共有\n",
    "- **一貫性**: 学習と推論での特徴量の一貫性を保証\n",
    "- **コラボレーション**: データチーム間のサイロを解消\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_store_schemas",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Feature Storeスキーマと権限の設定\n",
    "setup_sql = f\"\"\"\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "SET FS_ROLE_PRODUCER = '{solution_prefix}_FS_PRODUCER';\n",
    "SET FS_ROLE_CONSUMER = '{solution_prefix}_DATA_SCIENTIST';\n",
    "SET FS_DATABASE = '{solution_prefix}_PROD';\n",
    "SET FS_SCHEMA = 'FS_SCHEMA';\n",
    "SET SCHEMA_FQN = CONCAT($FS_DATABASE, '.', $FS_SCHEMA);\n",
    "SET FS_WAREHOUSE = '{solution_prefix}_DS_WH';\n",
    "\n",
    "-- スキーマの作成\n",
    "CREATE SCHEMA IF NOT EXISTS IDENTIFIER($FS_SCHEMA);\n",
    "\n",
    "-- ロール階層の構築\n",
    "GRANT ROLE IDENTIFIER($FS_ROLE_CONSUMER) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "\n",
    "-- PRODUCER ロール権限の付与\n",
    "GRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE DYNAMIC TABLE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE VIEW ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE TAG ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT APPLY TAG ON ACCOUNT TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "\n",
    "-- CONSUMER ロール権限の付与\n",
    "GRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,MONITOR ON ALL DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,REFERENCES ON FUTURE VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,REFERENCES ON ALL VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT USAGE ON WAREHOUSE IDENTIFIER($FS_WAREHOUSE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "\"\"\"\n",
    "\n",
    "# SQLコマンドの実行\n",
    "for sql_command in setup_sql.strip().split(';'):\n",
    "    if sql_command.strip():\n",
    "        try:\n",
    "            session.sql(sql_command.strip()).collect()\n",
    "        except Exception as e:\n",
    "            print(f\"警告: {sql_command[:50]}... の実行中にエラー: {e}\")\n",
    "\n",
    "print(\"✅ Feature Storeスキーマと権限の設定完了\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "fsdefinition",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Feature Store Producer ロールを使用してFeature Storeを初期化\n",
    "session.sql(f'USE ROLE {solution_prefix}_FS_PRODUCER').collect()\n",
    "session.sql(f'USE WAREHOUSE {solution_prefix}_DS_WH').collect()\n",
    "session.sql('USE SCHEMA FS_SCHEMA').collect()\n",
    "\n",
    "# Feature Storeの初期化\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(\"✅ Feature Store初期化完了\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "prepare_feature_md"
   },
   "source": [
    "## 5. 特徴量データの準備\n",
    "\n",
    "推薦システムに必要な特徴量を準備します：\n",
    "- 顧客特徴量（人口統計学的情報）\n",
    "- メニュー特徴量（商品情報）\n",
    "- 購入特徴量（購入履歴統計）\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_data_load",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# メニュー特徴量の取得\n",
    "menu_spdf = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        menu_type, \n",
    "        truck_brand_name, \n",
    "        menu_item_name, \n",
    "        item_category, \n",
    "        item_subcategory, \n",
    "        sale_price_usd \n",
    "    FROM raw_pos.menu \n",
    "    WHERE item_category != 'Beverage'\n",
    "\"\"\")\n",
    "\n",
    "# 顧客特徴量の取得\n",
    "cust_spdf = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        city, \n",
    "        country, \n",
    "        gender, \n",
    "        marital_status, \n",
    "        birthday_date, \n",
    "        DATEDIFF(year, birthday_date, CURRENT_DATE()) AS age \n",
    "    FROM raw_customer.customer_loyalty\n",
    "\"\"\")\n",
    "\n",
    "# 購入統計特徴量の計算\n",
    "avg_monthly_purchase = session.sql(f\"\"\"\n",
    "    SELECT  \n",
    "        customer_id, \n",
    "        ROUND(SUM(order_total) / (TIMESTAMPDIFF(MONTH, MIN(date), MAX(date)) + 1), 2) AS avg_monthly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "avg_weekly_purchase = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total) / (TIMESTAMPDIFF(WEEK, MIN(date), MAX(date)) + 1), 2) AS avg_weekly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "avg_yearly_purchase = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        ROUND(SUM(order_total)/(TIMESTAMPDIFF(YEAR, MIN(date), MAX(date)) + 1), 2) AS avg_yearly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# 購入統計の結合\n",
    "cust_avgs_spdf = avg_monthly_purchase.join(avg_weekly_purchase, \"CUSTOMER_ID\").join(avg_yearly_purchase, \"CUSTOMER_ID\")\n",
    "\n",
    "print(f\"✅ 特徴量データ準備完了\")\n",
    "print(f\"   メニュー特徴量: {menu_spdf.count()} レコード\")\n",
    "print(f\"   顧客特徴量: {cust_spdf.count()} レコード\")\n",
    "print(f\"   購入統計: {cust_avgs_spdf.count()} レコード\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "fs_entities",
    "collapsed": false
   },
   "source": "## 6. Feature Store エンティティ・特徴量ビュー・データセット構築\n\nFeature Storeでエンティティと特徴量ビューを作成し、学習用データセットを構築します。\n",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "register_entitiy",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# エンティティの作成と登録\n",
    "customer_entity = Entity(name=\"CustomerIds\", join_keys=[\"Customer_ID\"])\n",
    "fs.register_entity(customer_entity)\n",
    "\n",
    "menu_entity = Entity(name=\"Menu_ItemNames\", join_keys=[\"MENU_ITEM_NAME\"])\n",
    "fs.register_entity(menu_entity)\n",
    "\n",
    "purchase_entity = Entity(name=\"Purchase_Avgs\", join_keys=[\"Customer_ID\"])\n",
    "fs.register_entity(purchase_entity)\n",
    "\n",
    "# 特徴量ビューの作成と登録\n",
    "customer_fv = FeatureView(\n",
    "    name=\"CUSTOMER_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=cust_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=customer_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "menu_fv = FeatureView(\n",
    "    name=\"MENU_FEATURES\",\n",
    "    entities=[menu_entity],\n",
    "    feature_df=menu_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=menu_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "purchase_fv = FeatureView(\n",
    "    name=\"PURCHASE_FEATURES\",\n",
    "    entities=[purchase_entity],\n",
    "    feature_df=cust_avgs_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=purchase_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "# データサイエンティストロールに切り替え\n",
    "session.sql(f'USE ROLE {solution_prefix}_DATA_SCIENTIST').collect()\n",
    "\n",
    "# Feature Storeの再初期化\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\"\n",
    ")\n",
    "\n",
    "# 特徴量ビューの取得\n",
    "customer_fv = fs.get_feature_view(name='CUSTOMER_FEATURES', version='V1')\n",
    "menu_fv = fs.get_feature_view(name='MENU_FEATURES', version='V1')\n",
    "purchase_fv = fs.get_feature_view(name='PURCHASE_FEATURES', version='V1')\n",
    "\n",
    "print(\"✅ Feature Store エンティティ・特徴量ビュー作成完了\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "id": "e65e12a7-76a5-4e9d-9cf8-0ec22d27ea25",
   "metadata": {
    "name": "feature_engineering_md",
    "collapsed": false
   },
   "source": "### 6.1. 特徴量エンジニアリング\nこのモデルは、カテゴリ（スパース）特徴の埋め込みを作成する。カテゴリ値は一意な整数としてエンコードされる。前処理として、疎な特徴にはラベル符号化を、数値（密な）特徴には最小-最大スケーリングを適用します。\n\n**Snowflakeの特徴**: Snowpark ML Modeling API - 特徴エンジニアリングと前処理（GA） - 頻繁に使用されるscikit-learnの前処理関数を分散実行することで、パフォーマンスとスケーラビリティを向上。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_store1",
    "language": "python"
   },
   "outputs": [],
   "source": "# 特徴量の定義\nsparse_features = [\n    'MENU_ITEM_NAME', 'MENU_TYPE', 'TRUCK_BRAND_NAME', \n    'ITEM_CATEGORY', 'ITEM_SUBCATEGORY', 'CITY', 'COUNTRY', 'GENDER', 'MARITAL_STATUS'\n]\ndense_features = [\n    'SALE_PRICE_USD', 'AGE', 'AVG_MONTHLY_PURCHASE_AMOUNT', \n    'AVG_WEEKLY_PURCHASE_AMOUNT', 'AVG_YEARLY_PURCHASE_AMOUNT'\n]\nlabel_col = \"PURCHASED\"\n\n# データセット作成関数\ndef create_dataset(spine_df, name):\n    data = fs.generate_dataset(\n        name=name,\n        spine_df=spine_df,\n        features=[customer_fv, menu_fv, purchase_fv]\n    )\n    return data.read.to_snowpark_dataframe().drop(\"BIRTHDAY_DATE\")\n\n# 相互作用データセットの分割\ninteraction_df = session.table('analytics.loyalty_purchased_items')\ndatasets = interaction_df.random_split([0.1, 0.1, 0.8])\ntrain_df = create_dataset(datasets[0], \"pytr_train\")\nval_df = create_dataset(datasets[1], \"pytr_validation\")\n\n# 前処理パイプラインの作成\nfrom snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\n\npipeline_steps = []\nfor i, feat in enumerate(sparse_features):\n    pipeline_steps.append((f\"LE_{i+1}\", LabelEncoder(input_cols=[feat], output_cols=[feat])))\n\npipeline_steps.append((\"SCALER\", MinMaxScaler(\n    feature_range=(0, 1), input_cols=dense_features, output_cols=dense_features\n)))\n\npreprocessing_pipeline = Pipeline(steps=pipeline_steps)\ntrain_data = preprocessing_pipeline.fit(train_df).transform(train_df)\nval_data = preprocessing_pipeline.transform(val_df)\n\n# 前処理済みデータを一時テーブルに保存\ntrain_data.write.mode(\"overwrite\").save_as_table(\"ml.pytr_train_data\")\nval_data.write.mode(\"overwrite\").save_as_table(\"ml.pytr_val_data\")\n\nprint(\"✅ データセット構築・前処理完了\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "id": "fb1d963a-8bb6-46cc-8f20-721d6e51f191",
   "metadata": {
    "name": "save_pipeline_md",
    "collapsed": false
   },
   "source": "### パイプラインの保管\n保存されたパイプラインは推論における特徴量変換に使用されます。"
  },
  {
   "cell_type": "code",
   "id": "73f9d149-f2a6-4898-8987-dccd76e718f1",
   "metadata": {
    "language": "python",
    "name": "save_pipeline"
   },
   "outputs": [],
   "source": "# パイプラインを、一元的にアクセス可能な段階に保存する\npipeline_local_path = f'/tmp/dlrm_preprocessor_v1.joblib'\njoblib.dump(preprocessing_pipeline, open(pipeline_local_path, 'wb'))\nsession.file.put(pipeline_local_path, \n                 '@ML.ML_STAGE/dlrm_preprocessor_v1.joblib', \n                 auto_compress=False, \n                 overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89b5bd42-a69e-4f06-bbef-c1dd80b1cbd4",
   "metadata": {
    "language": "sql",
    "name": "udf_stage"
   },
   "outputs": [],
   "source": "USE SCHEMA ML;\nCREATE or replace STAGE UDF_STAGE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97ca5495-6b22-451b-8d8d-1bec990a8f3a",
   "metadata": {
    "language": "python",
    "name": "label_encorders"
   },
   "outputs": [],
   "source": "import json\n\ndata = train_df[dense_features + sparse_features + [label_col]]\ndata = data.with_columns(sparse_features,\n                        [F.col(c).cast(T.StringType()) for c in sparse_features])\n\ndef serialize_label_encoders(label_encoders):\n    serialized_label_encoders = {}\n    for feat, lbe in label_encoders.items():\n        serialized_label_encoders[feat] = {\n            'input_cols': lbe.input_cols,\n            'output_cols': lbe.output_cols,\n            'classes_': lbe.classes_.tolist()\n        }\n    return serialized_label_encoders\n\ndef save_label_encoders_to_stage(label_encoders, stage_name, dir_name):\n        serialized_label_encoders = json.dumps(label_encoders)\n        # Write serialized encoders to a local file first\n        with open('/tmp/label_encoders.json', 'w') as f:\n            f.write(serialized_label_encoders)\n        # Upload the local file to the Snowflake stage\n        session.file.put('/tmp/label_encoders.json', f'@{stage_name}/{dir_name}',auto_compress=False)\n        return f'Uploaded to @{stage_name}/{dir_name}'\n    \nlabel_encoders = {}\n\n# Iterate over each sparse feature\nfor feat in sparse_features:\n    # Initialize LabelEncoder for the current feature\n    lbe = LabelEncoder(input_cols=[feat], output_cols=[feat+'_ENCODED'],drop_input_cols=True)\n    \n    # Fit LabelEncoder to the data\n    lbe.fit(data)\n    \n    # Store the LabelEncoder object for reference\n    label_encoders[feat] = lbe\n    data = lbe.transform(data)\n# Serialize label encoders\nserialized_label_encoders = serialize_label_encoders(label_encoders)\nstage_name=\"UDF_STAGE\"\ndir_name=\"dlrm_label_encoders\"\n# Save serialized label encoders to a file\nwith open('/tmp/label_encoders.json', 'w') as f:\n    json.dump(serialized_label_encoders, f)\nsave_label_encoders_to_stage(serialized_label_encoders, stage_name, dir_name)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "define_models_md",
    "collapsed": false
   },
   "source": "## 7. モデル定義・学習\n\nこのPyTorchモデルは深層学習推薦モデル（DLRM）である。これは、各ロイヤリティ顧客に全てのメニューの推薦スコアを提供するために使用されている。\n\n- 埋め込み層はカテゴリ特徴を密なベクトルに変換する。\n- 数値的特徴は多層パーセプトロン（MLP）層を通して処理される。\n- 特徴相互作用層は入力特徴のペア間の複雑な関係を捉える。\n- 最後のdence層は推薦スコアを生成する。\n",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "model_definition1",
    "language": "python"
   },
   "outputs": [],
   "source": "# 深層学習推薦モデル（DLRM）の定義\nclass FeatureInteraction(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, inputs):\n        feature_dim = inputs.shape[1]\n        concat_features = inputs.view(-1, feature_dim, 1)\n        dot_products = torch.matmul(concat_features, concat_features.transpose(1, 2))\n        ones = torch.ones_like(dot_products) \n        mask = torch.triu(ones)\n        out_dim = feature_dim * (feature_dim + 1) // 2\n        flat_result = dot_products[mask.bool()]\n        reshape_result = flat_result.view(-1, out_dim)\n        return reshape_result\n\nclass DLRM(nn.Module):\n    \n    def __init__(self, sparse_feature_number, dense_feature_number, num_embeddings, embed_dim, bottom_mlp_dims, top_mlp_dims):\n        super(DLRM, self).__init__()\n        \n        self.embeddings = nn.EmbeddingBag(num_embeddings=num_embeddings, embedding_dim=embed_dim, mode='sum')\n        self.layer_feature_interaction = FeatureInteraction()\n        \n        self.bottom_mlp = torch.nn.Sequential(\n            torch.nn.Linear(dense_feature_number, bottom_mlp_dims[0]),\n            torch.nn.ReLU(),\n            torch.nn.Linear(bottom_mlp_dims[0], bottom_mlp_dims[1]),\n            torch.nn.ReLU()\n        )\n        \n        top_mlp_input_dim = (\n            (embed_dim + bottom_mlp_dims[1]) \n            * ((embed_dim + bottom_mlp_dims[1]) + 1) // 2 \n            + bottom_mlp_dims[1]\n         )\n\n        self.top_mlp = nn.Sequential(\n            nn.Linear(top_mlp_input_dim, top_mlp_dims[0]),\n            nn.ReLU(),\n            nn.Linear(top_mlp_dims[0], top_mlp_dims[1]),\n            nn.ReLU(),\n            nn.Linear(top_mlp_dims[1], 1)\n        )\n\n    def forward(self, x_sparse, x_dense):\n        # Embedding layer for categorical inputs\n        embed_x = self.embeddings(x_sparse)\n        # MLPs for numeric inputs\n        bottom_mlp_output = self.bottom_mlp(x_dense)\n        # Combine categical embeddings and MLP outputs\n        concat_first = torch.cat([bottom_mlp_output, embed_x], dim=-1)\n        # Get feature interactions\n        interaction = self.layer_feature_interaction(concat_first)\n        # Concat interaction outputs with MLP outputs\n        concat_second = torch.cat([interaction, bottom_mlp_output], dim=-1)\n        # MLP layers to output \n        output = self.top_mlp(concat_second)\n        return output\n\nprint(\"✅ DLRMモデル定義完了\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "markdown",
   "id": "e0f14988-5cab-4676-992b-4fd2f2ad1e73",
   "metadata": {
    "name": "model_training_md",
    "collapsed": false
   },
   "source": "### 7.1 モデルトレーニング\n\nモデルトレーニング機能は、各デバイスにモデルとデータを配置する。勾配は各トレーニングバッチで結合され、すべてのデバイスに伝搬される。各エポックの後、トレーニングと検証の損失がすべてのデバイスで平均化され、モデルの重みが保存されます。\n\n**Snowflakeの特長**: Snowpark ML Modeling API - PyTorch - Snowpark DataFrameからGPUデバイスに分散して実行。"
  },
  {
   "cell_type": "code",
   "id": "7665bd92-b665-470b-9287-c15a7c3d1ca3",
   "metadata": {
    "language": "python",
    "name": "training_env"
   },
   "outputs": [],
   "source": "# トレーニングデータのエポック数とレコード数を調整\nnum_epochs = 2\ntraining_sample = 100000",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "training",
    "language": "python"
   },
   "outputs": [],
   "source": "# 初期化\ndef setup(rank, world_size):\n    #処理グループの初期化\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    torch.manual_seed(42)\n\n# 分散学習関数とPyTorch Distributor実行\ndef distributed_training_function():\n    \"\"\"Container Runtime環境に最適化された分散学習関数\"\"\"\n    context = get_context()\n    rank = context.get_rank()\n    world_size = context.get_world_size()\n    # local_rank = context.get_local_rank()    \n    \n    # 分散学習の初期化\n    setup(rank, world_size)\n    # dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    # torch.cuda.set_device(local_rank)\n    # torch.manual_seed(42 + rank)\n    \n    batch_size = 256\n    \n    # データの取得\n    dataset_map = context.get_dataset_map()\n    training_data = dataset_map[\"train\"].get_shard().to_torch_datapipe(batch_size=batch_size, shuffle=True)\n    validation_data = dataset_map[\"val\"].get_shard().to_torch_datapipe(batch_size=batch_size, shuffle=False)\n    \n    train_loader = DataLoader(training_data, batch_size=None)\n    val_loader = DataLoader(validation_data, batch_size=None)\n    \n    # モデルの初期化\n    model = DLRM(\n        sparse_feature_number=len(sparse_features),\n        dense_feature_number=len(dense_features),\n        num_embeddings=142,\n        embed_dim=128,\n        bottom_mlp_dims=[256, 128],\n        top_mlp_dims=[128,128]\n    )\n    \n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    \n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    best_val_loss = float('inf')\n    \n    # 学習ループ\n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        ddp_model.train()\n        total_train_loss = 0.0\n        num_train_batches = 0\n        \n        for batch_idx, batch_data in enumerate(train_loader):\n            y = batch_data.pop(label_col).type(torch.float32).to(rank).squeeze()\n            \n            x_sparse = torch.stack(\n                [tensor.to(torch.int).squeeze() for key, tensor in batch_data.items() if key in sparse_features],\n                dim=1\n            )\n            x_dense = torch.stack(\n                [tensor.to(torch.float32).squeeze() for key, tensor in batch_data.items() if key in dense_features],\n                dim=1\n            )\n            \n            optimizer.zero_grad()\n            outputs = ddp_model(x_sparse, x_dense)\n            loss = criterion(outputs, y.unsqueeze(1))\n            loss.backward()\n            # torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            total_train_loss += loss.item()\n            num_train_batches += len(y)\n            \n            if batch_idx % 100 == 0 and rank == 0:\n                print(f\"📈 エポック {epoch+1}/{num_epochs}, バッチ {batch_idx}, 損失: {loss.item():.4f}\")\n        \n        # 平均学習損失の計算\n        avg_train_loss = total_train_loss / num_train_batches\n        train_loss_tensor = torch.tensor(avg_train_loss, device=rank)\n        dist.all_reduce(train_loss_tensor)\n        dist.barrier()\n        avg_train_loss = train_loss_tensor.item() / world_size\n        \n        # 検証フェーズ\n        ddp_model.eval()\n        total_val_loss = 0.0\n        num_val_batches = 0\n        \n        with torch.no_grad():\n            for batch_data in val_loader:\n                y_val = batch_data.pop(label_col).float().to(rank).squeeze()\n                x_sparse_val = torch.stack([\n                    batch_data[feat].long().squeeze() for feat in sparse_features\n                ], dim=1).to(rank)\n                x_dense_val = torch.stack([\n                    batch_data[feat].float().squeeze() for feat in dense_features\n                ], dim=1).to(rank)\n                \n                output_val = ddp_model(x_sparse_val, x_dense_val)\n                loss_val = criterion(output_val, y_val.unsqueeze(1))\n                total_val_loss += loss_val.item()\n                num_val_batches += 1\n        \n        # 平均検証損失の計算\n        avg_val_loss = total_val_loss / num_val_batches if num_val_batches > 0 else 0.0\n        val_loss_tensor = torch.tensor(avg_val_loss, device=rank)\n        dist.all_reduce(val_loss_tensor)\n        dist.barrier()\n        avg_val_loss = val_loss_tensor.item() / world_size\n        \n        if rank == 0:\n            epoch_time = time.time() - epoch_start_time\n            print(f\"🎯 エポック {epoch+1}/{num_epochs} 完了:\")\n            print(f\"   学習損失: {avg_train_loss:.4f}\")\n            print(f\"   検証損失: {avg_val_loss:.4f}\")\n            print(f\"   時間: {epoch_time:.2f}秒\")\n            \n            # 最良モデルの保存\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'train_loss': avg_train_loss,\n                    'val_loss': avg_val_loss\n                }, '/tmp/pytr_best_model.pth')\n                print(f\"💾 新しい最良モデル保存 (検証損失: {best_val_loss:.4f})\")\n    \n    dist.destroy_process_group()\n    \n    if rank == 0:\n        print(\"🎉 学習完了！\")\n        print(f\"🏆 最良検証損失: {best_val_loss:.4f}\")\n\n# PyTorch Distributorの設定と実行\npytorch_distributor = PyTorchDistributor(\n    train_func=distributed_training_function,\n    scaling_config=PyTorchScalingConfig(\n        num_nodes=1,\n        num_workers_per_node=1,\n        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=4, num_gpus=1)\n    )\n)\n\n# 学習データの準備\ntrain_table = session.table(\"ml.pytr_train_data\")\nval_table = session.table(\"ml.pytr_val_data\")\ntrain_connector = ShardedDataConnector.from_dataframe(train_table.limit(training_sample))\nval_connector = ShardedDataConnector.from_dataframe(val_table)\n\nprint(\"🏋️ 分散学習開始...\")\n\n# 分散学習の実行\ntraining_result = pytorch_distributor.run(\n    dataset_map={\"train\": train_connector, \"val\": val_connector}\n)\n\nprint(\"✅ 分散学習完了！\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "model_reg_md",
    "collapsed": false
   },
   "source": "## 8. モデルの登録とデプロイ\n\nモデルは Snowflake Model Registry に記録されます。ログに記録されたモデルは、Snowpark Container Services (SPCS)上で推論のためにデプロイされます。\n\n**Snowflakeの特長**： Snowflake Model Registry with SPCS deployment - 柔軟なコンピュート環境において、Snowflakeでモデルとそのメタデータを安全にデプロイ、管理します。\n\n![model_serving](https://docs.snowflake.com/en/_images/model-registry-spcs-deployment.png)\n",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "model_registory",
    "language": "python"
   },
   "outputs": [],
   "source": "# 学習済みモデルの読み込みとModel Registry登録\ndef load_trained_model(model_path):\n    model = DLRM(\n        sparse_feature_number=len(sparse_features),\n        dense_feature_number=len(dense_features),\n        num_embeddings=142, embed_dim=128,\n        bottom_mlp_dims=[256, 128], top_mlp_dims=[128, 128]\n    )\n    checkpoint = torch.load(model_path, map_location='cpu')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    return model, checkpoint\n\n# 学習済みモデルの読み込み\nmodel, checkpoint = load_trained_model('/tmp/pytr_best_model.pth')\n\n# Model Registryの初期化\nregistry = Registry(\n    session=session, \n    database_name=f\"{solution_prefix}_PROD\", \n    schema_name=\"REGISTRY\"\n)\n\n# サンプル入力データの準備\ntrain_table = session.table(\"ml.pytr_train_data\")\nsample_input = train_table.limit(1).to_pandas()\nx_sparse = torch.tensor(sample_input[sparse_features].values, dtype=torch.int)\nx_dense = torch.tensor(sample_input[dense_features].values, dtype=torch.float32)\n\n# モデルをレジストリに登録\nmodel_ref = registry.log_model(\n    model,\n    model_name=\"PyTorchRecModel\",\n    version_name=\"V1\",\n    pip_requirements=[\"torch==2.6.0\", \"torchvision==0.21.0\"],\n    sample_input_data=[x_sparse[0].unsqueeze(0), x_dense[0].unsqueeze(0)],\n    options={'relax_version': False, 'multiple_inputs': True}\n)\n\n# モデルメタデータの追加\nmodel_ref.description = \"PyTorch Deep Learning Recommendation System - Container Runtime最適化\"\n\n# 学習メトリクスの追加\nmodel_ref.set_metric(\"train_loss\", round(checkpoint['train_loss'], 4))\nmodel_ref.set_metric(\"val_loss\", round(checkpoint['val_loss'], 4))\nmodel_ref.set_metric(\"epochs_trained\", checkpoint['epoch'] + 1)\n\nprint(f\"✅ モデル登録完了: {model_ref.model_name} v{model_ref.version_name}\")\nprint(f\"   最終学習損失: {checkpoint['train_loss']:.4f}\")\nprint(f\"   最終検証損失: {checkpoint['val_loss']:.4f}\")\nprint(f\"   学習エポック: {checkpoint['epoch'] + 1}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "id": "9fe970a1-d01d-44d7-af41-3f0316c66de6",
   "metadata": {
    "language": "python",
    "name": "create_service"
   },
   "outputs": [],
   "source": "model_ref.create_service(\n    service_name=\"PYTORCH_RECOMMENDATION_SERVICE\",\n    service_compute_pool=f\"{solution_prefix}_DEPLOY_POOL\",\n    image_repo=f\"{solution_prefix}_PROD.REGISTRY.IMAGE_REPO\",\n    build_external_access_integration=f\"{solution_prefix}_CONDA_ACCESS_INTEGRATION\"\n)\nprint(\"✅ モデルがSPCSに正常にデプロイされました！\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "683688b4-eea0-45f9-9cb4-b1ee4acad557",
   "metadata": {
    "name": "verify_md",
    "collapsed": false
   },
   "source": "## 9. モデルの推論と検証\n\nテストデータの推論は、専用のコンピュートプール上で動作する SPCS にデプロイされたモデルを使用して完了する。テストデータのフィーチャーはフィーチャーストアからアクセスされ、前処理パイプラインが必要に応じてデータを変換します。\n\n**モデル出力**： モデルは入力特徴量に基づくスコアを出力する。スコアが高ければ高いほど、その顧客にとってそのメニューがよりおすすめであることを示します。モデルのパフォーマンスを評価するために、スコアからバイナリ予測が作成されます（スコアが0.5以上の場合は1、そうでない場合は0）。\n\n**スノーフレークの特徴：**\n\n- SPCS 上での推論（PuPr） - 専用のコンピュートプールを持つコンテナ環境にデプロイされたモデルに対して推論を実行します。\n- Snowpark ML Modeling API - Evaluation Metrics (GA) - 使用頻度の高い scikit-learn 前処理関数の分散実行により、パフォーマンスとスケーラビリティを向上。"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "inferense_verify",
    "language": "python"
   },
   "outputs": [],
   "source": "# テストデータでの推論と評価\ntest_df = create_dataset(datasets[2], \"pytr_test\")\ntest_df_subset = test_df.sample(n=5000)\ntest_data = preprocessing_pipeline.transform(test_df_subset)\ntest_data_pd = test_data.to_pandas()\n\n# 推論用入力データの準備\nsparse_input = torch.tensor(test_data_pd[sparse_features].values, dtype=torch.long)\ndense_input = torch.tensor(test_data_pd[dense_features].values, dtype=torch.float32)\n\n# SPCS推論\npredictions = model_ref.run(\n    [sparse_input, dense_input], \n    function_name=\"forward\", \n    service_name=\"PYTORCH_RECOMMENDATION_SERVICE\"\n)\npredictions['output_feature_0'] = predictions['output_feature_0'].apply(\n    lambda x: x[0] if isinstance(x, list) else float(x)\n)\nst.success(\"✅ SPCS推論完了\")\n    \n# 評価データフレームの作成\neval_df_pd = pd.concat([\n    test_data_pd[[\"CUSTOMER_ID\", \"MENU_ITEM_NAME\", \"PURCHASED\"]], \n    predictions.rename(columns={'output_feature_0': 'PREDICTION'})\n], axis=1).assign(\n    BINARY_PREDICTION=lambda df: np.where(df['PREDICTION'] >= 0.5, 1, 0)\n)\n\neval_df = session.create_dataframe(eval_df_pd)\n\n# 評価メトリクスの計算\nauc = roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\")\nprecision = precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\")\nrecall = recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\")\n\n# Streamlitでメトリクス表示\nst.subheader(\"📊 モデルパフォーマンス\")\ncols = st.columns(3)\ncols[0].metric(\"AUC\", round(auc, 3))\ncols[1].metric(\"Recall\", round(recall, 3))\ncols[2].metric(\"Precision\", round(precision, 3))\n\n# モデルレジストリにメトリクスを更新\nmodel_ref.set_metric(\"AUC\", round(auc, 4))\nmodel_ref.set_metric(\"Precision\", round(precision, 4))\nmodel_ref.set_metric(\"Recall\", round(recall, 4))\n\nf1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\nmodel_ref.set_metric(\"F1_Score\", round(f1_score, 4))\n",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "conclusion_md",
    "language": "python"
   },
   "outputs": [],
   "source": "# 🎉 Snowflake ML クイックスタート完了サマリー\n\nprint(\"🎉 Snowflake ML クイックスタート完了！\")\nprint(\"=\"*60)\nprint(f\"📊 最終モデルパフォーマンス:\")\nprint(f\"   AUC: {auc:.4f}\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall: {recall:.4f}\")\nprint(f\"   F1 Score: {f1_score:.4f}\")\nprint(f\"\")\nprint(f\"🏆 モデル情報:\")\nprint(f\"   名前: {model_ref.model_name}\")\nprint(f\"   バージョン: {model_ref.version_name}\")\nprint(f\"   学習エポック: {checkpoint['epoch'] + 1}\")\nprint(f\"   最終検証損失: {checkpoint['val_loss']:.4f}\")\nprint(f\"\")\nprint(f\"🔧 使用した技術:\")\nprint(f\"   - Snowflake Notebooks on Container Runtime\")\nprint(f\"   - PyTorch Distributor (分散学習)\")\nprint(f\"   - Snowflake Feature Store\")\nprint(f\"   - Snowflake Model Registry\")\nprint(f\"   - SPCS デプロイメント\")\nprint(\"=\"*60)\nprint(\"✅ すべてのコンポーネントが正常に動作しました！\")\nprint(\"\")\nprint(\"🚀 次のステップ:\")\nprint(\"   1. ハイパーパラメータチューニングでパフォーマンス向上\")\nprint(\"   2. A/Bテストによる本番環境でのモデル比較\")\nprint(\"   3. リアルタイム監視でデータドリフト検出\")\nprint(\"   4. 自動再学習パイプラインの構築\")\nprint(\"\")\nprint(\"📖 Container Runtime for ML の詳細:\")\nprint(\"   https://docs.snowflake.com/en/developer-guide/snowflake-ml/notebooks-on-spcs\")\n\n# セッションのクローズ（Notebook環境では通常クローズしない）\n# session.close()\n",
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "id": "f3c5e1ab-bb67-467a-839c-ec0da132f8f4",
   "metadata": {
    "name": "cleanup_md",
    "collapsed": false
   },
   "source": "### Clean up"
  },
  {
   "cell_type": "code",
   "id": "ef92dda0-739d-481e-b77d-f608fef6a1f5",
   "metadata": {
    "language": "python",
    "name": "cleanup"
   },
   "outputs": [],
   "source": "# # SPCS Model Service\n# model_ref.delete_service(\n#     service_name=\"PYTORCH_RECOMMENDATION_SERVICE\"\n# )\n\n# # Model\n# registry.delete_model(\n#     model_name=\"PyTorchRecModel\"\n# )\n\n# # Feature View\n# fs.delete_feature_view(customer_fv)\n# fs.delete_feature_view(menu_fv)\n# fs.delete_feature_view(purchase_fv)\n\n\n# # Feature Entiry\n# fs.delete_entity(\"CustomerIds\")\n# fs.delete_entity(\"Menu_ItemNames\")\n# fs.delete_entity(\"Purchase_Avgs\")",
   "execution_count": null
  }
 ]
}