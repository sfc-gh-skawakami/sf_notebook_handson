{
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "caqv4aausnhucdf2ygeg",
   "authorId": "4469144185903",
   "authorName": "SKAWAKAMI",
   "authorEmail": "shinichi.kawakami@snowflake.com",
   "sessionId": "b87f64e3-38f6-490a-b48f-8be1d049ab8a",
   "lastEditTime": 1756442900874
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "intro_md",
    "collapsed": false
   },
   "source": "# Snowflake ML with PyTorch Distributor - ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰\n\nã“ã®Notebookã§ã¯ã€PyTorch Distributorã‚’ä½¿ç”¨ã—ãŸSnowflake MLã®åŒ…æ‹¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’æ´»ç”¨ã—ã¦ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®MLãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚\n\nã“ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³ã¯ã€Tasty Bytesã®ãƒ•ãƒ¼ãƒ‰ãƒˆãƒ©ãƒƒã‚¯ãŒè²©å£²ã™ã‚‹å„ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«ã¤ã„ã¦ã€ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£é¡§å®¢ã«ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¹ã‚³ã‚¢ã‚’æä¾›ã™ã‚‹ã€‚ã“ã®å‡ºåŠ›ã¯ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸã‚¢ã‚¦ãƒˆãƒªãƒ¼ãƒã€é¡§å®¢ãŒè¨ªå•ã™ã‚‹ãƒˆãƒ©ãƒƒã‚¯ãƒ–ãƒ©ãƒ³ãƒ‰ã®å¢—åŠ ã€æ¥­ç¸¾ä¸æŒ¯ã®ãƒˆãƒ©ãƒƒã‚¯ã¸ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã®å¢—åŠ ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚\nãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¯ã€GPUãƒ‡ãƒã‚¤ã‚¹é–“ã§ã®åˆ†æ•£å­¦ç¿’ã‚’æ´»ç”¨ã—ã¦ãŠã‚Šã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã¨ãƒ‡ãƒ—ãƒ­ã‚¤ã¯ã€ä»¥ä¸‹ã®Snowflakeæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ç°¡ç´ åŒ–ãŠã‚ˆã³åˆç†åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼š\n\n## å­¦ç¿’å†…å®¹\n\n- **Snowflake Notebooks with GPU Container Runtime (GA)**: Snowflake Notebooks\n- **Snowflake Feature Store (GA)** : ç‰¹å¾´é‡ã®ä¸€å…ƒç®¡ç†ã¨å†åˆ©ç”¨ \n- **Snowflake Modeling API (GA)** : Preprocessing, Training (PyTorch API), Evaluation\n- **Snowflake Model Registry (GA)** : ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ\n- **Model Deployment from Registry to SPCS (GA)** : ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹\n\n## æ§‹ç¯‰ã™ã‚‹ã‚‚ã®\n\nä»¥ä¸‹ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®æ·±å±¤å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ï¼š\n- GPUåŠ é€Ÿã«ã‚ˆã‚‹åˆ†æ•£PyTorchå­¦ç¿’\n- Snowflake Feature Storeã‚’ä½¿ç”¨ã—ãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\n- Snowpark Container Services (SPCS)ã¸ã®ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ\n- åŒ…æ‹¬çš„ãªãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨èª¬æ˜å¯èƒ½æ€§\n",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "setup_md"
   },
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨GPUç¢ºèª\n",
    "\n",
    "Container Runtimeç’°å¢ƒã§ã¯ã€å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒäº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãšGPUã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "setup",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®ç¢ºèª\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"âœ… åˆ©ç”¨å¯èƒ½ãªGPUãƒ‡ãƒã‚¤ã‚¹æ•°: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        print(f\"   ãƒ‡ãƒã‚¤ã‚¹ {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
    "    torch.cuda.set_device(0)\n",
    "    print(f\"   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚GPUè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "## 2. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "Container Runtimeç’°å¢ƒã§ã¯ã€MLé–‹ç™ºã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒäº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "lib_import",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Snowflakeãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "# Snowflake ML\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    FeatureView,\n",
    "    Entity,\n",
    "    CreationMode\n",
    ")\n",
    "from snowflake.ml.registry import Registry\n",
    "from snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.distributors.pytorch import (\n",
    "    PyTorchDistributor, \n",
    "    PyTorchScalingConfig, \n",
    "    WorkerResourceConfig,\n",
    "    get_context\n",
    ")\n",
    "from snowflake.ml.data.sharded_data_connector import ShardedDataConnector\n",
    "from snowflake.ml.modeling.metrics import (\n",
    "    roc_auc_score,  \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "print(\"âœ… ã™ã¹ã¦ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸï¼\")\n",
    "print(f\"PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}\")\n",
    "print(f\"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "initialize_md"
   },
   "source": [
    "## 3. Snowflakeã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–\n",
    "\n",
    "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªSnowflakeã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—ã—ã€ã‚¯ã‚¨ãƒªã‚¿ã‚°ã‚’è¨­å®šã—ã¾ã™ã€‚\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000005"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "initialize",
    "language": "python"
   },
   "outputs": [],
   "source": "# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å–å¾—\nsession = get_active_session()\n\n# ã‚¯ã‚¨ãƒªã‚¿ã‚°ã®è¨­å®šï¼ˆãƒ‡ãƒãƒƒã‚°ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã«å½¹ç«‹ã¡ã¾ã™ï¼‰\nsession.query_tag = {\n    \"origin\": \"pytorch_ml_quickstart\",\n    \"name\": \"end_to_end_recommendation_system\",\n    \"version\": {\"major\": 1, \"minor\": 0},\n    \"attributes\": {\"is_quickstart\": 1, \"source\": \"notebook\"}\n}\n\n# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã®å–å¾—\ndb = str(session.get_current_database().strip('\"'))\nsolution_prefix = (db.upper()).split('_PROD')[0]\n\nprint(f\"âœ… Snowflakeã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–å®Œäº†\")\nprint(f\"   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db}\")\nprint(f\"   ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹: {solution_prefix}\")\nprint(f\"   ç¾åœ¨ã®ãƒ­ãƒ¼ãƒ«: {session.get_current_role()}\")\nprint(f\"   ç¾åœ¨ã®ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹: {session.get_current_warehouse()}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000006"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "setup_feature_store_md"
   },
   "source": [
    "## 4. Feature Storeã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "Snowflake Feature Storeã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã¨æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã®ç‰¹å¾´é‡ã®ä½œæˆã€ä¿å­˜ã€ç®¡ç†ã‚’ã‚ˆã‚Šç°¡å˜ã§åŠ¹ç‡çš„ã«ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "**ä¸»è¦ãªãƒ¡ãƒªãƒƒãƒˆ:**\n",
    "- **ä¸€å…ƒåŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ç®¡ç†**: ç‰¹å¾´é‡ã‚’ä¸€ç®‡æ‰€ã§ä¿å­˜ãƒ»ç®¡ç†\n",
    "- **ç‰¹å¾´é‡ã®å†åˆ©ç”¨**: ãƒãƒ¼ãƒ ã‚„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã§ã®ç‰¹å¾´é‡ã®å…±æœ‰\n",
    "- **ä¸€è²«æ€§**: å­¦ç¿’ã¨æ¨è«–ã§ã®ç‰¹å¾´é‡ã®ä¸€è²«æ€§ã‚’ä¿è¨¼\n",
    "- **ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒ é–“ã®ã‚µã‚¤ãƒ­ã‚’è§£æ¶ˆ\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_store_schemas",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Feature Storeã‚¹ã‚­ãƒ¼ãƒã¨æ¨©é™ã®è¨­å®š\n",
    "setup_sql = f\"\"\"\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "SET FS_ROLE_PRODUCER = '{solution_prefix}_FS_PRODUCER';\n",
    "SET FS_ROLE_CONSUMER = '{solution_prefix}_DATA_SCIENTIST';\n",
    "SET FS_DATABASE = '{solution_prefix}_PROD';\n",
    "SET FS_SCHEMA = 'FS_SCHEMA';\n",
    "SET SCHEMA_FQN = CONCAT($FS_DATABASE, '.', $FS_SCHEMA);\n",
    "SET FS_WAREHOUSE = '{solution_prefix}_DS_WH';\n",
    "\n",
    "-- ã‚¹ã‚­ãƒ¼ãƒã®ä½œæˆ\n",
    "CREATE SCHEMA IF NOT EXISTS IDENTIFIER($FS_SCHEMA);\n",
    "\n",
    "-- ãƒ­ãƒ¼ãƒ«éšå±¤ã®æ§‹ç¯‰\n",
    "GRANT ROLE IDENTIFIER($FS_ROLE_CONSUMER) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "\n",
    "-- PRODUCER ãƒ­ãƒ¼ãƒ«æ¨©é™ã®ä»˜ä¸\n",
    "GRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE DYNAMIC TABLE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE VIEW ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE TAG ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "GRANT APPLY TAG ON ACCOUNT TO ROLE IDENTIFIER($FS_ROLE_PRODUCER);\n",
    "\n",
    "-- CONSUMER ãƒ­ãƒ¼ãƒ«æ¨©é™ã®ä»˜ä¸\n",
    "GRANT USAGE ON DATABASE IDENTIFIER($FS_DATABASE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT USAGE ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,MONITOR ON ALL DYNAMIC TABLES IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,REFERENCES ON FUTURE VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT SELECT,REFERENCES ON ALL VIEWS IN SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT CREATE DATASET ON SCHEMA IDENTIFIER($SCHEMA_FQN) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "GRANT USAGE ON WAREHOUSE IDENTIFIER($FS_WAREHOUSE) TO ROLE IDENTIFIER($FS_ROLE_CONSUMER);\n",
    "\"\"\"\n",
    "\n",
    "# SQLã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ\n",
    "for sql_command in setup_sql.strip().split(';'):\n",
    "    if sql_command.strip():\n",
    "        try:\n",
    "            session.sql(sql_command.strip()).collect()\n",
    "        except Exception as e:\n",
    "            print(f\"è­¦å‘Š: {sql_command[:50]}... ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "print(\"âœ… Feature Storeã‚¹ã‚­ãƒ¼ãƒã¨æ¨©é™ã®è¨­å®šå®Œäº†\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "fsdefinition",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Feature Store Producer ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦Feature Storeã‚’åˆæœŸåŒ–\n",
    "session.sql(f'USE ROLE {solution_prefix}_FS_PRODUCER').collect()\n",
    "session.sql(f'USE WAREHOUSE {solution_prefix}_DS_WH').collect()\n",
    "session.sql('USE SCHEMA FS_SCHEMA').collect()\n",
    "\n",
    "# Feature Storeã®åˆæœŸåŒ–\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "print(\"âœ… Feature StoreåˆæœŸåŒ–å®Œäº†\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "prepare_feature_md"
   },
   "source": [
    "## 5. ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "\n",
    "æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«å¿…è¦ãªç‰¹å¾´é‡ã‚’æº–å‚™ã—ã¾ã™ï¼š\n",
    "- é¡§å®¢ç‰¹å¾´é‡ï¼ˆäººå£çµ±è¨ˆå­¦çš„æƒ…å ±ï¼‰\n",
    "- ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç‰¹å¾´é‡ï¼ˆå•†å“æƒ…å ±ï¼‰\n",
    "- è³¼å…¥ç‰¹å¾´é‡ï¼ˆè³¼å…¥å±¥æ­´çµ±è¨ˆï¼‰\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000010"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_data_load",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç‰¹å¾´é‡ã®å–å¾—\n",
    "menu_spdf = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        menu_type, \n",
    "        truck_brand_name, \n",
    "        menu_item_name, \n",
    "        item_category, \n",
    "        item_subcategory, \n",
    "        sale_price_usd \n",
    "    FROM raw_pos.menu \n",
    "    WHERE item_category != 'Beverage'\n",
    "\"\"\")\n",
    "\n",
    "# é¡§å®¢ç‰¹å¾´é‡ã®å–å¾—\n",
    "cust_spdf = session.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        city, \n",
    "        country, \n",
    "        gender, \n",
    "        marital_status, \n",
    "        birthday_date, \n",
    "        DATEDIFF(year, birthday_date, CURRENT_DATE()) AS age \n",
    "    FROM raw_customer.customer_loyalty\n",
    "\"\"\")\n",
    "\n",
    "# è³¼å…¥çµ±è¨ˆç‰¹å¾´é‡ã®è¨ˆç®—\n",
    "avg_monthly_purchase = session.sql(f\"\"\"\n",
    "    SELECT  \n",
    "        customer_id, \n",
    "        ROUND(SUM(order_total) / (TIMESTAMPDIFF(MONTH, MIN(date), MAX(date)) + 1), 2) AS avg_monthly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "avg_weekly_purchase = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        ROUND(SUM(order_total) / (TIMESTAMPDIFF(WEEK, MIN(date), MAX(date)) + 1), 2) AS avg_weekly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "avg_yearly_purchase = session.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id, \n",
    "        ROUND(SUM(order_total)/(TIMESTAMPDIFF(YEAR, MIN(date), MAX(date)) + 1), 2) AS avg_yearly_purchase_amount \n",
    "    FROM {solution_prefix}_PROD.ANALYTICS.ORDERS_V \n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# è³¼å…¥çµ±è¨ˆã®çµåˆ\n",
    "cust_avgs_spdf = avg_monthly_purchase.join(avg_weekly_purchase, \"CUSTOMER_ID\").join(avg_yearly_purchase, \"CUSTOMER_ID\")\n",
    "\n",
    "print(f\"âœ… ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†\")\n",
    "print(f\"   ãƒ¡ãƒ‹ãƒ¥ãƒ¼ç‰¹å¾´é‡: {menu_spdf.count()} ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n",
    "print(f\"   é¡§å®¢ç‰¹å¾´é‡: {cust_spdf.count()} ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n",
    "print(f\"   è³¼å…¥çµ±è¨ˆ: {cust_avgs_spdf.count()} ãƒ¬ã‚³ãƒ¼ãƒ‰\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "fs_entities",
    "collapsed": false
   },
   "source": "## 6. Feature Store ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ»ç‰¹å¾´é‡ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰\n\nFeature Storeã§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ç‰¹å¾´é‡ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆã—ã€å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚\n",
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "register_entitiy",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä½œæˆã¨ç™»éŒ²\n",
    "customer_entity = Entity(name=\"CustomerIds\", join_keys=[\"Customer_ID\"])\n",
    "fs.register_entity(customer_entity)\n",
    "\n",
    "menu_entity = Entity(name=\"Menu_ItemNames\", join_keys=[\"MENU_ITEM_NAME\"])\n",
    "fs.register_entity(menu_entity)\n",
    "\n",
    "purchase_entity = Entity(name=\"Purchase_Avgs\", join_keys=[\"Customer_ID\"])\n",
    "fs.register_entity(purchase_entity)\n",
    "\n",
    "# ç‰¹å¾´é‡ãƒ“ãƒ¥ãƒ¼ã®ä½œæˆã¨ç™»éŒ²\n",
    "customer_fv = FeatureView(\n",
    "    name=\"CUSTOMER_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=cust_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=customer_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "menu_fv = FeatureView(\n",
    "    name=\"MENU_FEATURES\",\n",
    "    entities=[menu_entity],\n",
    "    feature_df=menu_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=menu_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "purchase_fv = FeatureView(\n",
    "    name=\"PURCHASE_FEATURES\",\n",
    "    entities=[purchase_entity],\n",
    "    feature_df=cust_avgs_spdf,\n",
    "    refresh_freq=\"1 day\"\n",
    ")\n",
    "fs.register_feature_view(feature_view=purchase_fv, version=\"V1\", block=True, overwrite=True)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆãƒ­ãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "session.sql(f'USE ROLE {solution_prefix}_DATA_SCIENTIST').collect()\n",
    "\n",
    "# Feature Storeã®å†åˆæœŸåŒ–\n",
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=f\"{solution_prefix}_PROD\",\n",
    "    name=\"FS_SCHEMA\",\n",
    "    default_warehouse=f\"{solution_prefix}_DS_WH\"\n",
    ")\n",
    "\n",
    "# ç‰¹å¾´é‡ãƒ“ãƒ¥ãƒ¼ã®å–å¾—\n",
    "customer_fv = fs.get_feature_view(name='CUSTOMER_FEATURES', version='V1')\n",
    "menu_fv = fs.get_feature_view(name='MENU_FEATURES', version='V1')\n",
    "purchase_fv = fs.get_feature_view(name='PURCHASE_FEATURES', version='V1')\n",
    "\n",
    "print(\"âœ… Feature Store ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ»ç‰¹å¾´é‡ãƒ“ãƒ¥ãƒ¼ä½œæˆå®Œäº†\")\n"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "markdown",
   "id": "e65e12a7-76a5-4e9d-9cf8-0ec22d27ea25",
   "metadata": {
    "name": "feature_engineering_md",
    "collapsed": false
   },
   "source": "### 6.1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°\nã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰ç‰¹å¾´ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã™ã‚‹ã€‚ã‚«ãƒ†ã‚´ãƒªå€¤ã¯ä¸€æ„ãªæ•´æ•°ã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚å‰å‡¦ç†ã¨ã—ã¦ã€ç–ãªç‰¹å¾´ã«ã¯ãƒ©ãƒ™ãƒ«ç¬¦å·åŒ–ã‚’ã€æ•°å€¤ï¼ˆå¯†ãªï¼‰ç‰¹å¾´ã«ã¯æœ€å°-æœ€å¤§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ã¾ã™ã€‚\n\n**Snowflakeã®ç‰¹å¾´**: Snowpark ML Modeling API - ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã¨å‰å‡¦ç†ï¼ˆGAï¼‰ - é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹scikit-learnã®å‰å‡¦ç†é–¢æ•°ã‚’åˆ†æ•£å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "feature_store1",
    "language": "python"
   },
   "outputs": [],
   "source": "# ç‰¹å¾´é‡ã®å®šç¾©\nsparse_features = [\n    'MENU_ITEM_NAME', 'MENU_TYPE', 'TRUCK_BRAND_NAME', \n    'ITEM_CATEGORY', 'ITEM_SUBCATEGORY', 'CITY', 'COUNTRY', 'GENDER', 'MARITAL_STATUS'\n]\ndense_features = [\n    'SALE_PRICE_USD', 'AGE', 'AVG_MONTHLY_PURCHASE_AMOUNT', \n    'AVG_WEEKLY_PURCHASE_AMOUNT', 'AVG_YEARLY_PURCHASE_AMOUNT'\n]\nlabel_col = \"PURCHASED\"\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–¢æ•°\ndef create_dataset(spine_df, name):\n    data = fs.generate_dataset(\n        name=name,\n        spine_df=spine_df,\n        features=[customer_fv, menu_fv, purchase_fv]\n    )\n    return data.read.to_snowpark_dataframe().drop(\"BIRTHDAY_DATE\")\n\n# ç›¸äº’ä½œç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰²\ninteraction_df = session.table('analytics.loyalty_purchased_items')\ndatasets = interaction_df.random_split([0.1, 0.1, 0.8])\ntrain_df = create_dataset(datasets[0], \"pytr_train\")\nval_df = create_dataset(datasets[1], \"pytr_validation\")\n\n# å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ\nfrom snowflake.ml.modeling.preprocessing import LabelEncoder, MinMaxScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\n\npipeline_steps = []\nfor i, feat in enumerate(sparse_features):\n    pipeline_steps.append((f\"LE_{i+1}\", LabelEncoder(input_cols=[feat], output_cols=[feat])))\n\npipeline_steps.append((\"SCALER\", MinMaxScaler(\n    feature_range=(0, 1), input_cols=dense_features, output_cols=dense_features\n)))\n\npreprocessing_pipeline = Pipeline(steps=pipeline_steps)\ntrain_data = preprocessing_pipeline.fit(train_df).transform(train_df)\nval_data = preprocessing_pipeline.transform(val_df)\n\n# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜\ntrain_data.write.mode(\"overwrite\").save_as_table(\"ml.pytr_train_data\")\nval_data.write.mode(\"overwrite\").save_as_table(\"ml.pytr_val_data\")\n\nprint(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ãƒ»å‰å‡¦ç†å®Œäº†\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000015"
  },
  {
   "cell_type": "markdown",
   "id": "fb1d963a-8bb6-46cc-8f20-721d6e51f191",
   "metadata": {
    "name": "save_pipeline_md",
    "collapsed": false
   },
   "source": "### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¿ç®¡\nä¿å­˜ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯æ¨è«–ã«ãŠã‘ã‚‹ç‰¹å¾´é‡å¤‰æ›ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
  },
  {
   "cell_type": "code",
   "id": "73f9d149-f2a6-4898-8987-dccd76e718f1",
   "metadata": {
    "language": "python",
    "name": "save_pipeline"
   },
   "outputs": [],
   "source": "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã€ä¸€å…ƒçš„ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªæ®µéšã«ä¿å­˜ã™ã‚‹\npipeline_local_path = f'/tmp/dlrm_preprocessor_v1.joblib'\njoblib.dump(preprocessing_pipeline, open(pipeline_local_path, 'wb'))\nsession.file.put(pipeline_local_path, \n                 '@ML.ML_STAGE/dlrm_preprocessor_v1.joblib', \n                 auto_compress=False, \n                 overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89b5bd42-a69e-4f06-bbef-c1dd80b1cbd4",
   "metadata": {
    "language": "sql",
    "name": "udf_stage"
   },
   "outputs": [],
   "source": "USE SCHEMA ML;\nCREATE or replace STAGE UDF_STAGE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97ca5495-6b22-451b-8d8d-1bec990a8f3a",
   "metadata": {
    "language": "python",
    "name": "label_encorders"
   },
   "outputs": [],
   "source": "import json\n\ndata = train_df[dense_features + sparse_features + [label_col]]\ndata = data.with_columns(sparse_features,\n                        [F.col(c).cast(T.StringType()) for c in sparse_features])\n\ndef serialize_label_encoders(label_encoders):\n    serialized_label_encoders = {}\n    for feat, lbe in label_encoders.items():\n        serialized_label_encoders[feat] = {\n            'input_cols': lbe.input_cols,\n            'output_cols': lbe.output_cols,\n            'classes_': lbe.classes_.tolist()\n        }\n    return serialized_label_encoders\n\ndef save_label_encoders_to_stage(label_encoders, stage_name, dir_name):\n        serialized_label_encoders = json.dumps(label_encoders)\n        # Write serialized encoders to a local file first\n        with open('/tmp/label_encoders.json', 'w') as f:\n            f.write(serialized_label_encoders)\n        # Upload the local file to the Snowflake stage\n        session.file.put('/tmp/label_encoders.json', f'@{stage_name}/{dir_name}',auto_compress=False)\n        return f'Uploaded to @{stage_name}/{dir_name}'\n    \nlabel_encoders = {}\n\n# Iterate over each sparse feature\nfor feat in sparse_features:\n    # Initialize LabelEncoder for the current feature\n    lbe = LabelEncoder(input_cols=[feat], output_cols=[feat+'_ENCODED'],drop_input_cols=True)\n    \n    # Fit LabelEncoder to the data\n    lbe.fit(data)\n    \n    # Store the LabelEncoder object for reference\n    label_encoders[feat] = lbe\n    data = lbe.transform(data)\n# Serialize label encoders\nserialized_label_encoders = serialize_label_encoders(label_encoders)\nstage_name=\"UDF_STAGE\"\ndir_name=\"dlrm_label_encoders\"\n# Save serialized label encoders to a file\nwith open('/tmp/label_encoders.json', 'w') as f:\n    json.dump(serialized_label_encoders, f)\nsave_label_encoders_to_stage(serialized_label_encoders, stage_name, dir_name)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "define_models_md",
    "collapsed": false
   },
   "source": "## 7. ãƒ¢ãƒ‡ãƒ«å®šç¾©ãƒ»å­¦ç¿’\n\nã“ã®PyTorchãƒ¢ãƒ‡ãƒ«ã¯æ·±å±¤å­¦ç¿’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ï¼ˆDLRMï¼‰ã§ã‚ã‚‹ã€‚ã“ã‚Œã¯ã€å„ãƒ­ã‚¤ãƒ¤ãƒªãƒ†ã‚£é¡§å®¢ã«å…¨ã¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®æ¨è–¦ã‚¹ã‚³ã‚¢ã‚’æä¾›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚\n\n- åŸ‹ã‚è¾¼ã¿å±¤ã¯ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´ã‚’å¯†ãªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ã€‚\n- æ•°å€¤çš„ç‰¹å¾´ã¯å¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼ˆMLPï¼‰å±¤ã‚’é€šã—ã¦å‡¦ç†ã•ã‚Œã‚‹ã€‚\n- ç‰¹å¾´ç›¸äº’ä½œç”¨å±¤ã¯å…¥åŠ›ç‰¹å¾´ã®ãƒšã‚¢é–“ã®è¤‡é›‘ãªé–¢ä¿‚ã‚’æ‰ãˆã‚‹ã€‚\n- æœ€å¾Œã®denceå±¤ã¯æ¨è–¦ã‚¹ã‚³ã‚¢ã‚’ç”Ÿæˆã™ã‚‹ã€‚\n",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "model_definition1",
    "language": "python"
   },
   "outputs": [],
   "source": "# æ·±å±¤å­¦ç¿’æ¨è–¦ãƒ¢ãƒ‡ãƒ«ï¼ˆDLRMï¼‰ã®å®šç¾©\nclass FeatureInteraction(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, inputs):\n        feature_dim = inputs.shape[1]\n        concat_features = inputs.view(-1, feature_dim, 1)\n        dot_products = torch.matmul(concat_features, concat_features.transpose(1, 2))\n        ones = torch.ones_like(dot_products) \n        mask = torch.triu(ones)\n        out_dim = feature_dim * (feature_dim + 1) // 2\n        flat_result = dot_products[mask.bool()]\n        reshape_result = flat_result.view(-1, out_dim)\n        return reshape_result\n\nclass DLRM(nn.Module):\n    \n    def __init__(self, sparse_feature_number, dense_feature_number, num_embeddings, embed_dim, bottom_mlp_dims, top_mlp_dims):\n        super(DLRM, self).__init__()\n        \n        self.embeddings = nn.EmbeddingBag(num_embeddings=num_embeddings, embedding_dim=embed_dim, mode='sum')\n        self.layer_feature_interaction = FeatureInteraction()\n        \n        self.bottom_mlp = torch.nn.Sequential(\n            torch.nn.Linear(dense_feature_number, bottom_mlp_dims[0]),\n            torch.nn.ReLU(),\n            torch.nn.Linear(bottom_mlp_dims[0], bottom_mlp_dims[1]),\n            torch.nn.ReLU()\n        )\n        \n        top_mlp_input_dim = (\n            (embed_dim + bottom_mlp_dims[1]) \n            * ((embed_dim + bottom_mlp_dims[1]) + 1) // 2 \n            + bottom_mlp_dims[1]\n         )\n\n        self.top_mlp = nn.Sequential(\n            nn.Linear(top_mlp_input_dim, top_mlp_dims[0]),\n            nn.ReLU(),\n            nn.Linear(top_mlp_dims[0], top_mlp_dims[1]),\n            nn.ReLU(),\n            nn.Linear(top_mlp_dims[1], 1)\n        )\n\n    def forward(self, x_sparse, x_dense):\n        # Embedding layer for categorical inputs\n        embed_x = self.embeddings(x_sparse)\n        # MLPs for numeric inputs\n        bottom_mlp_output = self.bottom_mlp(x_dense)\n        # Combine categical embeddings and MLP outputs\n        concat_first = torch.cat([bottom_mlp_output, embed_x], dim=-1)\n        # Get feature interactions\n        interaction = self.layer_feature_interaction(concat_first)\n        # Concat interaction outputs with MLP outputs\n        concat_second = torch.cat([interaction, bottom_mlp_output], dim=-1)\n        # MLP layers to output \n        output = self.top_mlp(concat_second)\n        return output\n\nprint(\"âœ… DLRMãƒ¢ãƒ‡ãƒ«å®šç¾©å®Œäº†\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "markdown",
   "id": "e0f14988-5cab-4676-992b-4fd2f2ad1e73",
   "metadata": {
    "name": "model_training_md",
    "collapsed": false
   },
   "source": "### 7.1 ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n\nãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã¯ã€å„ãƒ‡ãƒã‚¤ã‚¹ã«ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’é…ç½®ã™ã‚‹ã€‚å‹¾é…ã¯å„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒƒãƒã§çµåˆã•ã‚Œã€ã™ã¹ã¦ã®ãƒ‡ãƒã‚¤ã‚¹ã«ä¼æ¬ã•ã‚Œã‚‹ã€‚å„ã‚¨ãƒãƒƒã‚¯ã®å¾Œã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ã®æå¤±ãŒã™ã¹ã¦ã®ãƒ‡ãƒã‚¤ã‚¹ã§å¹³å‡åŒ–ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚\n\n**Snowflakeã®ç‰¹é•·**: Snowpark ML Modeling API - PyTorch - Snowpark DataFrameã‹ã‚‰GPUãƒ‡ãƒã‚¤ã‚¹ã«åˆ†æ•£ã—ã¦å®Ÿè¡Œã€‚"
  },
  {
   "cell_type": "code",
   "id": "7665bd92-b665-470b-9287-c15a7c3d1ca3",
   "metadata": {
    "language": "python",
    "name": "training_env"
   },
   "outputs": [],
   "source": "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒãƒƒã‚¯æ•°ã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’èª¿æ•´\nnum_epochs = 2\ntraining_sample = 100000",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "training",
    "language": "python"
   },
   "outputs": [],
   "source": "# åˆæœŸåŒ–\ndef setup(rank, world_size):\n    #å‡¦ç†ã‚°ãƒ«ãƒ¼ãƒ—ã®åˆæœŸåŒ–\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    torch.manual_seed(42)\n\n# åˆ†æ•£å­¦ç¿’é–¢æ•°ã¨PyTorch Distributorå®Ÿè¡Œ\ndef distributed_training_function():\n    \"\"\"Container Runtimeç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸåˆ†æ•£å­¦ç¿’é–¢æ•°\"\"\"\n    context = get_context()\n    rank = context.get_rank()\n    world_size = context.get_world_size()\n    # local_rank = context.get_local_rank()    \n    \n    # åˆ†æ•£å­¦ç¿’ã®åˆæœŸåŒ–\n    setup(rank, world_size)\n    # dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    # torch.cuda.set_device(local_rank)\n    # torch.manual_seed(42 + rank)\n    \n    batch_size = 256\n    \n    # ãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n    dataset_map = context.get_dataset_map()\n    training_data = dataset_map[\"train\"].get_shard().to_torch_datapipe(batch_size=batch_size, shuffle=True)\n    validation_data = dataset_map[\"val\"].get_shard().to_torch_datapipe(batch_size=batch_size, shuffle=False)\n    \n    train_loader = DataLoader(training_data, batch_size=None)\n    val_loader = DataLoader(validation_data, batch_size=None)\n    \n    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–\n    model = DLRM(\n        sparse_feature_number=len(sparse_features),\n        dense_feature_number=len(dense_features),\n        num_embeddings=142,\n        embed_dim=128,\n        bottom_mlp_dims=[256, 128],\n        top_mlp_dims=[128,128]\n    )\n    \n    model = model.to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n    \n    criterion = torch.nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.AdamW(ddp_model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    best_val_loss = float('inf')\n    \n    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        ddp_model.train()\n        total_train_loss = 0.0\n        num_train_batches = 0\n        \n        for batch_idx, batch_data in enumerate(train_loader):\n            y = batch_data.pop(label_col).type(torch.float32).to(rank).squeeze()\n            \n            x_sparse = torch.stack(\n                [tensor.to(torch.int).squeeze() for key, tensor in batch_data.items() if key in sparse_features],\n                dim=1\n            )\n            x_dense = torch.stack(\n                [tensor.to(torch.float32).squeeze() for key, tensor in batch_data.items() if key in dense_features],\n                dim=1\n            )\n            \n            optimizer.zero_grad()\n            outputs = ddp_model(x_sparse, x_dense)\n            loss = criterion(outputs, y.unsqueeze(1))\n            loss.backward()\n            # torch.nn.utils.clip_grad_norm_(ddp_model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            total_train_loss += loss.item()\n            num_train_batches += len(y)\n            \n            if batch_idx % 100 == 0 and rank == 0:\n                print(f\"ğŸ“ˆ ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{num_epochs}, ãƒãƒƒãƒ {batch_idx}, æå¤±: {loss.item():.4f}\")\n        \n        # å¹³å‡å­¦ç¿’æå¤±ã®è¨ˆç®—\n        avg_train_loss = total_train_loss / num_train_batches\n        train_loss_tensor = torch.tensor(avg_train_loss, device=rank)\n        dist.all_reduce(train_loss_tensor)\n        dist.barrier()\n        avg_train_loss = train_loss_tensor.item() / world_size\n        \n        # æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º\n        ddp_model.eval()\n        total_val_loss = 0.0\n        num_val_batches = 0\n        \n        with torch.no_grad():\n            for batch_data in val_loader:\n                y_val = batch_data.pop(label_col).float().to(rank).squeeze()\n                x_sparse_val = torch.stack([\n                    batch_data[feat].long().squeeze() for feat in sparse_features\n                ], dim=1).to(rank)\n                x_dense_val = torch.stack([\n                    batch_data[feat].float().squeeze() for feat in dense_features\n                ], dim=1).to(rank)\n                \n                output_val = ddp_model(x_sparse_val, x_dense_val)\n                loss_val = criterion(output_val, y_val.unsqueeze(1))\n                total_val_loss += loss_val.item()\n                num_val_batches += 1\n        \n        # å¹³å‡æ¤œè¨¼æå¤±ã®è¨ˆç®—\n        avg_val_loss = total_val_loss / num_val_batches if num_val_batches > 0 else 0.0\n        val_loss_tensor = torch.tensor(avg_val_loss, device=rank)\n        dist.all_reduce(val_loss_tensor)\n        dist.barrier()\n        avg_val_loss = val_loss_tensor.item() / world_size\n        \n        if rank == 0:\n            epoch_time = time.time() - epoch_start_time\n            print(f\"ğŸ¯ ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{num_epochs} å®Œäº†:\")\n            print(f\"   å­¦ç¿’æå¤±: {avg_train_loss:.4f}\")\n            print(f\"   æ¤œè¨¼æå¤±: {avg_val_loss:.4f}\")\n            print(f\"   æ™‚é–“: {epoch_time:.2f}ç§’\")\n            \n            # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'train_loss': avg_train_loss,\n                    'val_loss': avg_val_loss\n                }, '/tmp/pytr_best_model.pth')\n                print(f\"ğŸ’¾ æ–°ã—ã„æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ä¿å­˜ (æ¤œè¨¼æå¤±: {best_val_loss:.4f})\")\n    \n    dist.destroy_process_group()\n    \n    if rank == 0:\n        print(\"ğŸ‰ å­¦ç¿’å®Œäº†ï¼\")\n        print(f\"ğŸ† æœ€è‰¯æ¤œè¨¼æå¤±: {best_val_loss:.4f}\")\n\n# PyTorch Distributorã®è¨­å®šã¨å®Ÿè¡Œ\npytorch_distributor = PyTorchDistributor(\n    train_func=distributed_training_function,\n    scaling_config=PyTorchScalingConfig(\n        num_nodes=1,\n        num_workers_per_node=1,\n        resource_requirements_per_worker=WorkerResourceConfig(num_cpus=4, num_gpus=1)\n    )\n)\n\n# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\ntrain_table = session.table(\"ml.pytr_train_data\")\nval_table = session.table(\"ml.pytr_val_data\")\ntrain_connector = ShardedDataConnector.from_dataframe(train_table.limit(training_sample))\nval_connector = ShardedDataConnector.from_dataframe(val_table)\n\nprint(\"ğŸ‹ï¸ åˆ†æ•£å­¦ç¿’é–‹å§‹...\")\n\n# åˆ†æ•£å­¦ç¿’ã®å®Ÿè¡Œ\ntraining_result = pytorch_distributor.run(\n    dataset_map={\"train\": train_connector, \"val\": val_connector}\n)\n\nprint(\"âœ… åˆ†æ•£å­¦ç¿’å®Œäº†ï¼\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000017"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "model_reg_md",
    "collapsed": false
   },
   "source": "## 8. ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²ã¨ãƒ‡ãƒ—ãƒ­ã‚¤\n\nãƒ¢ãƒ‡ãƒ«ã¯ Snowflake Model Registry ã«è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€Snowpark Container Services (SPCS)ä¸Šã§æ¨è«–ã®ãŸã‚ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¾ã™ã€‚\n\n**Snowflakeã®ç‰¹é•·**ï¼š Snowflake Model Registry with SPCS deployment - æŸ”è»Ÿãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆç’°å¢ƒã«ãŠã„ã¦ã€Snowflakeã§ãƒ¢ãƒ‡ãƒ«ã¨ãã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã€ç®¡ç†ã—ã¾ã™ã€‚\n\n![model_serving](https://docs.snowflake.com/en/_images/model-registry-spcs-deployment.png)\n",
   "id": "ce110000-1111-2222-3333-ffffff000018"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "model_registory",
    "language": "python"
   },
   "outputs": [],
   "source": "# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨Model Registryç™»éŒ²\ndef load_trained_model(model_path):\n    model = DLRM(\n        sparse_feature_number=len(sparse_features),\n        dense_feature_number=len(dense_features),\n        num_embeddings=142, embed_dim=128,\n        bottom_mlp_dims=[256, 128], top_mlp_dims=[128, 128]\n    )\n    checkpoint = torch.load(model_path, map_location='cpu')\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    return model, checkpoint\n\n# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\nmodel, checkpoint = load_trained_model('/tmp/pytr_best_model.pth')\n\n# Model Registryã®åˆæœŸåŒ–\nregistry = Registry(\n    session=session, \n    database_name=f\"{solution_prefix}_PROD\", \n    schema_name=\"REGISTRY\"\n)\n\n# ã‚µãƒ³ãƒ—ãƒ«å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\ntrain_table = session.table(\"ml.pytr_train_data\")\nsample_input = train_table.limit(1).to_pandas()\nx_sparse = torch.tensor(sample_input[sparse_features].values, dtype=torch.int)\nx_dense = torch.tensor(sample_input[dense_features].values, dtype=torch.float32)\n\n# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²\nmodel_ref = registry.log_model(\n    model,\n    model_name=\"PyTorchRecModel\",\n    version_name=\"V1\",\n    pip_requirements=[\"torch==2.6.0\", \"torchvision==0.21.0\"],\n    sample_input_data=[x_sparse[0].unsqueeze(0), x_dense[0].unsqueeze(0)],\n    options={'relax_version': False, 'multiple_inputs': True}\n)\n\n# ãƒ¢ãƒ‡ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ \nmodel_ref.description = \"PyTorch Deep Learning Recommendation System - Container Runtimeæœ€é©åŒ–\"\n\n# å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ \nmodel_ref.set_metric(\"train_loss\", round(checkpoint['train_loss'], 4))\nmodel_ref.set_metric(\"val_loss\", round(checkpoint['val_loss'], 4))\nmodel_ref.set_metric(\"epochs_trained\", checkpoint['epoch'] + 1)\n\nprint(f\"âœ… ãƒ¢ãƒ‡ãƒ«ç™»éŒ²å®Œäº†: {model_ref.model_name} v{model_ref.version_name}\")\nprint(f\"   æœ€çµ‚å­¦ç¿’æå¤±: {checkpoint['train_loss']:.4f}\")\nprint(f\"   æœ€çµ‚æ¤œè¨¼æå¤±: {checkpoint['val_loss']:.4f}\")\nprint(f\"   å­¦ç¿’ã‚¨ãƒãƒƒã‚¯: {checkpoint['epoch'] + 1}\")\n",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "id": "9fe970a1-d01d-44d7-af41-3f0316c66de6",
   "metadata": {
    "language": "python",
    "name": "create_service"
   },
   "outputs": [],
   "source": "model_ref.create_service(\n    service_name=\"PYTORCH_RECOMMENDATION_SERVICE\",\n    service_compute_pool=f\"{solution_prefix}_DEPLOY_POOL\",\n    image_repo=f\"{solution_prefix}_PROD.REGISTRY.IMAGE_REPO\",\n    build_external_access_integration=f\"{solution_prefix}_CONDA_ACCESS_INTEGRATION\"\n)\nprint(\"âœ… ãƒ¢ãƒ‡ãƒ«ãŒSPCSã«æ­£å¸¸ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¾ã—ãŸï¼\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "683688b4-eea0-45f9-9cb4-b1ee4acad557",
   "metadata": {
    "name": "verify_md",
    "collapsed": false
   },
   "source": "## 9. ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã¨æ¤œè¨¼\n\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¨è«–ã¯ã€å°‚ç”¨ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆãƒ—ãƒ¼ãƒ«ä¸Šã§å‹•ä½œã™ã‚‹ SPCS ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦å®Œäº†ã™ã‚‹ã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã¯ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã€å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒå¿…è¦ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã™ã€‚\n\n**ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›**ï¼š ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›ç‰¹å¾´é‡ã«åŸºã¥ãã‚¹ã‚³ã‚¢ã‚’å‡ºåŠ›ã™ã‚‹ã€‚ã‚¹ã‚³ã‚¢ãŒé«˜ã‘ã‚Œã°é«˜ã„ã»ã©ã€ãã®é¡§å®¢ã«ã¨ã£ã¦ãã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãŒã‚ˆã‚ŠãŠã™ã™ã‚ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€ã‚¹ã‚³ã‚¢ã‹ã‚‰ãƒã‚¤ãƒŠãƒªäºˆæ¸¬ãŒä½œæˆã•ã‚Œã¾ã™ï¼ˆã‚¹ã‚³ã‚¢ãŒ0.5ä»¥ä¸Šã®å ´åˆã¯1ã€ãã†ã§ãªã„å ´åˆã¯0ï¼‰ã€‚\n\n**ã‚¹ãƒãƒ¼ãƒ•ãƒ¬ãƒ¼ã‚¯ã®ç‰¹å¾´ï¼š**\n\n- SPCS ä¸Šã§ã®æ¨è«–ï¼ˆPuPrï¼‰ - å°‚ç”¨ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆãƒ—ãƒ¼ãƒ«ã‚’æŒã¤ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n- Snowpark ML Modeling API - Evaluation Metrics (GA) - ä½¿ç”¨é »åº¦ã®é«˜ã„ scikit-learn å‰å‡¦ç†é–¢æ•°ã®åˆ†æ•£å®Ÿè¡Œã«ã‚ˆã‚Šã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "inferense_verify",
    "language": "python"
   },
   "outputs": [],
   "source": "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ¨è«–ã¨è©•ä¾¡\ntest_df = create_dataset(datasets[2], \"pytr_test\")\ntest_df_subset = test_df.sample(n=5000)\ntest_data = preprocessing_pipeline.transform(test_df_subset)\ntest_data_pd = test_data.to_pandas()\n\n# æ¨è«–ç”¨å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\nsparse_input = torch.tensor(test_data_pd[sparse_features].values, dtype=torch.long)\ndense_input = torch.tensor(test_data_pd[dense_features].values, dtype=torch.float32)\n\n# SPCSæ¨è«–\npredictions = model_ref.run(\n    [sparse_input, dense_input], \n    function_name=\"forward\", \n    service_name=\"PYTORCH_RECOMMENDATION_SERVICE\"\n)\npredictions['output_feature_0'] = predictions['output_feature_0'].apply(\n    lambda x: x[0] if isinstance(x, list) else float(x)\n)\nst.success(\"âœ… SPCSæ¨è«–å®Œäº†\")\n    \n# è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆ\neval_df_pd = pd.concat([\n    test_data_pd[[\"CUSTOMER_ID\", \"MENU_ITEM_NAME\", \"PURCHASED\"]], \n    predictions.rename(columns={'output_feature_0': 'PREDICTION'})\n], axis=1).assign(\n    BINARY_PREDICTION=lambda df: np.where(df['PREDICTION'] >= 0.5, 1, 0)\n)\n\neval_df = session.create_dataframe(eval_df_pd)\n\n# è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—\nauc = roc_auc_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_score_col_names=\"PREDICTION\")\nprecision = precision_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\")\nrecall = recall_score(df=eval_df, y_true_col_names=\"PURCHASED\", y_pred_col_names=\"BINARY_PREDICTION\")\n\n# Streamlitã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º\nst.subheader(\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\")\ncols = st.columns(3)\ncols[0].metric(\"AUC\", round(auc, 3))\ncols[1].metric(\"Recall\", round(recall, 3))\ncols[2].metric(\"Precision\", round(precision, 3))\n\n# ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°\nmodel_ref.set_metric(\"AUC\", round(auc, 4))\nmodel_ref.set_metric(\"Precision\", round(precision, 4))\nmodel_ref.set_metric(\"Recall\", round(recall, 4))\n\nf1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\nmodel_ref.set_metric(\"F1_Score\", round(f1_score, 4))\n",
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "conclusion_md",
    "language": "python"
   },
   "outputs": [],
   "source": "# ğŸ‰ Snowflake ML ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆå®Œäº†ã‚µãƒãƒªãƒ¼\n\nprint(\"ğŸ‰ Snowflake ML ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆå®Œäº†ï¼\")\nprint(\"=\"*60)\nprint(f\"ğŸ“Š æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:\")\nprint(f\"   AUC: {auc:.4f}\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall: {recall:.4f}\")\nprint(f\"   F1 Score: {f1_score:.4f}\")\nprint(f\"\")\nprint(f\"ğŸ† ãƒ¢ãƒ‡ãƒ«æƒ…å ±:\")\nprint(f\"   åå‰: {model_ref.model_name}\")\nprint(f\"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {model_ref.version_name}\")\nprint(f\"   å­¦ç¿’ã‚¨ãƒãƒƒã‚¯: {checkpoint['epoch'] + 1}\")\nprint(f\"   æœ€çµ‚æ¤œè¨¼æå¤±: {checkpoint['val_loss']:.4f}\")\nprint(f\"\")\nprint(f\"ğŸ”§ ä½¿ç”¨ã—ãŸæŠ€è¡“:\")\nprint(f\"   - Snowflake Notebooks on Container Runtime\")\nprint(f\"   - PyTorch Distributor (åˆ†æ•£å­¦ç¿’)\")\nprint(f\"   - Snowflake Feature Store\")\nprint(f\"   - Snowflake Model Registry\")\nprint(f\"   - SPCS ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ\")\nprint(\"=\"*60)\nprint(\"âœ… ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¾ã—ãŸï¼\")\nprint(\"\")\nprint(\"ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\nprint(\"   1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š\")\nprint(\"   2. A/Bãƒ†ã‚¹ãƒˆã«ã‚ˆã‚‹æœ¬ç•ªç’°å¢ƒã§ã®ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ\")\nprint(\"   3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã§ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º\")\nprint(\"   4. è‡ªå‹•å†å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\")\nprint(\"\")\nprint(\"ğŸ“– Container Runtime for ML ã®è©³ç´°:\")\nprint(\"   https://docs.snowflake.com/en/developer-guide/snowflake-ml/notebooks-on-spcs\")\n\n# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¯ãƒ­ãƒ¼ã‚ºï¼ˆNotebookç’°å¢ƒã§ã¯é€šå¸¸ã‚¯ãƒ­ãƒ¼ã‚ºã—ãªã„ï¼‰\n# session.close()\n",
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "id": "f3c5e1ab-bb67-467a-839c-ec0da132f8f4",
   "metadata": {
    "name": "cleanup_md",
    "collapsed": false
   },
   "source": "### Clean up"
  },
  {
   "cell_type": "code",
   "id": "ef92dda0-739d-481e-b77d-f608fef6a1f5",
   "metadata": {
    "language": "python",
    "name": "cleanup"
   },
   "outputs": [],
   "source": "# # SPCS Model Service\n# model_ref.delete_service(\n#     service_name=\"PYTORCH_RECOMMENDATION_SERVICE\"\n# )\n\n# # Model\n# registry.delete_model(\n#     model_name=\"PyTorchRecModel\"\n# )\n\n# # Feature View\n# fs.delete_feature_view(customer_fv)\n# fs.delete_feature_view(menu_fv)\n# fs.delete_feature_view(purchase_fv)\n\n\n# # Feature Entiry\n# fs.delete_entity(\"CustomerIds\")\n# fs.delete_entity(\"Menu_ItemNames\")\n# fs.delete_entity(\"Purchase_Avgs\")",
   "execution_count": null
  }
 ]
}